{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(210, 255, 153, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/chart-network.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">How to Build LLM Apps that can See Hear Speak</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    <b class=\"fa fa-solid fa-info-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Note</b></p>\n",
        "        <p>This tutorial is meant for Standard & Premium Workspaces. You can't run this with a Free Starter Workspace due to restrictions on Storage. Create a Workspace using +group in the left nav & select Standard for this notebook. Gallery notebooks tagged with \"Starter\" are suitable to run on a Free Starter Workspace </p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"architecture\"></a>\n",
        "\n",
        "# Demo Architecture\n",
        "![SingleStore LLM App](https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/bltb25c874a947f70f9/65aa3b99573161fd44cc7b7d/singlestore-llm-app.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"contents\"></a>\n",
        "# Contents\n",
        "- [Demo Architecture](#architecture)\n",
        "- [Step 1: SingleStore DDLs](#ddl)\n",
        "- [Step 2: Packages and imports](#imports)\n",
        "- [Step 3: Ingest from data sources](#ingest)\n",
        "- [Step 4: Connect SingleStore to Open AI's LLM with Langchain](#connect_s2)\n",
        "- [Step 5: Add Voice Recognition and Speech](#speech)\n",
        "- [Step 6: Tying it together with Image data](#image)\n",
        "- [Conclusion](#conclusion)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"ddl\"></a>\n",
        "- [Back to Contents](#contents)\n",
        "## Setup SingleStore DDLs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "Create and use the database llm_webinar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n        </tr>\n    </thead>\n    <tbody>\n    </tbody>\n</table>",
            "text/plain": "++\n||\n++\n++"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "DROP DATABASE IF EXISTS llm_webinar;\n",
        "CREATE DATABASE llm_webinar;"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> Make sure to select a database from the drop-down menu at the top of this notebook. It updates the <tt>connection_url</tt>  to connect to that database.</p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "Create tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n        </tr>\n    </thead>\n    <tbody>\n    </tbody>\n</table>",
            "text/plain": "++\n||\n++\n++"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "CREATE TABLE `stockTable` (\n",
        "    `ticker` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,\n",
        "    `created_at` datetime DEFAULT NULL,\n",
        "    `open` float DEFAULT NULL,\n",
        "    `high` float DEFAULT NULL,\n",
        "    `low` float DEFAULT NULL,\n",
        "    `close` float DEFAULT NULL,\n",
        "    `volume` int(11) DEFAULT NULL,\n",
        "    SORT KEY (ticker, created_at desc),\n",
        "    SHARD KEY (ticker)\n",
        ");\n",
        "\n",
        "CREATE TABLE newsSentiment (\n",
        "    title TEXT CHARACTER SET utf8mb4,\n",
        "    url TEXT,\n",
        "    time_published DATETIME,\n",
        "    authors TEXT,\n",
        "    summary TEXT CHARACTER SET utf8mb4,\n",
        "    banner_image TEXT,\n",
        "    source TEXT,\n",
        "    category_within_source TEXT,\n",
        "    source_domain TEXT,\n",
        "    topic TEXT,\n",
        "    topic_relevance_score TEXT,\n",
        "    overall_sentiment_score REAL,\n",
        "    overall_sentiment_label TEXT,\n",
        "    `ticker` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,\n",
        "    ticker_relevance_score DECIMAL(10, 6),\n",
        "    ticker_sentiment_score DECIMAL(10, 6),\n",
        "    ticker_sentiment_label TEXT,\n",
        "    SORT KEY (`ticker`,`time_published` DESC),\n",
        "    SHARD KEY `__SHARDKEY` (`ticker`,`time_published` DESC),\n",
        "    KEY(ticker) USING HASH,\n",
        "    KEY(authors) USING HASH,\n",
        "    KEY(source) USING HASH,\n",
        "    KEY(overall_sentiment_label) USING HASH,\n",
        "    KEY(ticker_sentiment_label) USING HASH\n",
        ");\n",
        "\n",
        "CREATE ROWSTORE REFERENCE TABLE companyInfo (\n",
        "    ticker VARCHAR(10) PRIMARY KEY,\n",
        "    AssetType VARCHAR(50),\n",
        "    Name VARCHAR(100),\n",
        "    Description TEXT,\n",
        "    CIK VARCHAR(10),\n",
        "    Exchange VARCHAR(10),\n",
        "    Currency VARCHAR(10),\n",
        "    Country VARCHAR(50),\n",
        "    Sector VARCHAR(50),\n",
        "    Industry VARCHAR(250),\n",
        "    Address VARCHAR(100),\n",
        "    FiscalYearEnd VARCHAR(20),\n",
        "    LatestQuarter DATE,\n",
        "    MarketCapitalization BIGINT,\n",
        "    EBITDA BIGINT,\n",
        "    PERatio DECIMAL(10, 2),\n",
        "    PEGRatio DECIMAL(10, 3),\n",
        "    BookValue DECIMAL(10, 2),\n",
        "    DividendPerShare DECIMAL(10, 2),\n",
        "    DividendYield DECIMAL(10, 4),\n",
        "    EPS DECIMAL(10, 2),\n",
        "    RevenuePerShareTTM DECIMAL(10, 2),\n",
        "    ProfitMargin DECIMAL(10, 4),\n",
        "    OperatingMarginTTM DECIMAL(10, 4),\n",
        "    ReturnOnAssetsTTM DECIMAL(10, 4),\n",
        "    ReturnOnEquityTTM DECIMAL(10, 4),\n",
        "    RevenueTTM BIGINT,\n",
        "    GrossProfitTTM BIGINT,\n",
        "    DilutedEPSTTM DECIMAL(10, 2),\n",
        "    QuarterlyEarningsGrowthYOY DECIMAL(10, 3),\n",
        "    QuarterlyRevenueGrowthYOY DECIMAL(10, 3),\n",
        "    AnalystTargetPrice DECIMAL(10, 2),\n",
        "    TrailingPE DECIMAL(10, 2),\n",
        "    ForwardPE DECIMAL(10, 2),\n",
        "    PriceToSalesRatioTTM DECIMAL(10, 3),\n",
        "    PriceToBookRatio DECIMAL(10, 2),\n",
        "    EVToRevenue DECIMAL(10, 3),\n",
        "    EVToEBITDA DECIMAL(10, 2),\n",
        "    Beta DECIMAL(10, 3),\n",
        "    52WeekHigh DECIMAL(10, 2),\n",
        "    52WeekLow DECIMAL(10, 2),\n",
        "    50DayMovingAverage DECIMAL(10, 2),\n",
        "    200DayMovingAverage DECIMAL(10, 2),\n",
        "    SharesOutstanding BIGINT,\n",
        "    DividendDate DATE,\n",
        "    ExDividendDate DATE\n",
        ");\n",
        "\n",
        "CREATE TABLE `embeddings` (\n",
        "    `id` bigint(11) NOT NULL AUTO_INCREMENT,\n",
        "    `category` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,\n",
        "    `question` longtext CHARACTER SET utf8 COLLATE utf8_general_ci,\n",
        "    `question_embedding` longblob,\n",
        "    `answer` longtext CHARACTER SET utf8 COLLATE utf8_general_ci,\n",
        "    `answer_embedding` longblob,\n",
        "    `created_at` datetime DEFAULT NULL,\n",
        "    UNIQUE KEY `PRIMARY` (`id`) USING HASH,\n",
        "    SHARD KEY `__SHARDKEY` (`id`),\n",
        "    KEY `category` (`category`) USING HASH,\n",
        "    SORT KEY `__UNORDERED` (`created_at` DESC)\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n            <th>Tables_in_llm_webinar</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>companyInfo</td>\n        </tr>\n        <tr>\n            <td>embeddings</td>\n        </tr>\n        <tr>\n            <td>newsSentiment</td>\n        </tr>\n        <tr>\n            <td>stockTable</td>\n        </tr>\n    </tbody>\n</table>",
            "text/plain": "+-----------------------+\n| Tables_in_llm_webinar |\n+-----------------------+\n|      companyInfo      |\n|       embeddings      |\n|     newsSentiment     |\n|       stockTable      |\n+-----------------------+"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "SHOW TABLES;"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"imports\"></a>\n",
        "- [Back to Contents](#contents)\n",
        "## Install packages and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: elevenlabs==0.2.27 in /opt/conda/lib/python3.11/site-packages (0.2.27)\nRequirement already satisfied: openai==1.32.0 in /opt/conda/lib/python3.11/site-packages (1.32.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (1.14.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\nRequirement already satisfied: langchain==0.2 in /opt/conda/lib/python3.11/site-packages (0.2.0)\nRequirement already satisfied: langchain-openai==0.1.20 in /opt/conda/lib/python3.11/site-packages (0.1.20)\nCollecting langchain-community\n  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.11/site-packages (from elevenlabs==0.2.27) (2.8.2)\nRequirement already satisfied: ipython>=7.0 in /opt/conda/lib/python3.11/site-packages (from elevenlabs==0.2.27) (8.16.1)\nRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from elevenlabs==0.2.27) (2.31.0)\nRequirement already satisfied: websockets>=11.0 in /opt/conda/lib/python3.11/site-packages (from elevenlabs==0.2.27) (12.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai==1.32.0) (4.0.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai==1.32.0) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai==1.32.0) (0.27.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai==1.32.0) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai==1.32.0) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai==1.32.0) (4.8.0)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (2.0.21)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (3.10.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (0.6.7)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (0.2.28)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (0.2.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (0.1.96)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (1.26.2)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain==0.2) (8.2.3)\nRequirement already satisfied: tiktoken<1,>=0.7 in /opt/conda/lib/python3.11/site-packages (from langchain-openai==0.1.20) (0.7.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2) (2.3.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2) (1.9.4)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.32.0) (3.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2) (0.9.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.32.0) (2023.7.22)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.32.0) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.32.0) (0.14.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (0.1.6)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (3.0.39)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (2.16.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (5.11.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.0->elevenlabs==0.2.27) (4.8.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2) (1.33)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2) (3.10.6)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->elevenlabs==0.2.27) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->elevenlabs==0.2.27) (2.20.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->elevenlabs==0.2.27) (3.3.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->elevenlabs==0.2.27) (1.26.16)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2) (3.0.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.20) (2024.5.15)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.0->elevenlabs==0.2.27) (0.8.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2) (2.4)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.0->elevenlabs==0.2.27) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.0->elevenlabs==0.2.27) (0.2.8)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2) (1.0.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.0->elevenlabs==0.2.27) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.0->elevenlabs==0.2.27) (2.4.0)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.0->elevenlabs==0.2.27) (0.2.2)\nDownloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: langchain-community\nSuccessfully installed langchain-community-0.2.4\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "source": [
        "%pip install elevenlabs==0.2.27 openai==1.32.0 matplotlib scipy scikit-learn langchain==0.2 langchain-openai==0.1.20 langchain-community==0.2.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import getpass\n",
        "import numpy as np\n",
        "import openai\n",
        "import requests\n",
        "import singlestoredb as s2\n",
        "import time\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from langchain.sql_database import SQLDatabase\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.agents import create_sql_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2c760381-e46f-43d2-a6cb-263abc6d471f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embeddings(inputs: list[str], model: str = 'text-embedding-ada-002') -> list[str]:\n",
        "    \"\"\"Return list of embeddings.\"\"\"\n",
        "    return [x.embedding for x in client.embeddings.create(input=inputs, model=model).data]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Set API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": "enter alphavantage apikey here \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nenter openai apikey here \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nenter elevenlabs apikey here \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
        }
      ],
      "source": [
        "alpha_vantage_apikey = getpass.getpass(\"enter alphavantage apikey here\")\n",
        "openai_apikey = getpass.getpass(\"enter openai apikey here\")\n",
        "elevenlabs_apikey = getpass.getpass(\"enter elevenlabs apikey here\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"ingest\"></a>\n",
        "- [Back to Contents](#contents)\n",
        "## Ingest from data sources"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Bring past two months of stock data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up connection to SingleStore and the ticker list\n",
        "s2_conn = s2.connect(connection_url)\n",
        "ticker_list = ['TSLA', 'AMZN', 'PLTR']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "['2024-08', '2024-07']\nTSLA\n2024-08\n2024-07\nAMZN\n2024-08\n2024-07\nPLTR\n2024-08\n2024-07\n"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_past_months(num_months):\n",
        "    today = datetime.today()\n",
        "    months = []\n",
        "\n",
        "    for months_ago in range(0, num_months):\n",
        "        target_date = today - relativedelta(months=months_ago)\n",
        "        months.append(target_date.strftime('%Y-%m'))\n",
        "\n",
        "    return months\n",
        "\n",
        "num_months = 2  # Number of months\n",
        "year_month_list = get_past_months(num_months)\n",
        "print(year_month_list)\n",
        "\n",
        "# pull intraday data for each stock and write to SingleStore\n",
        "for ticker in ticker_list:\n",
        "    print(ticker)\n",
        "    data_list = []\n",
        "    for year_month in year_month_list:\n",
        "        print(year_month)\n",
        "\n",
        "        intraday_price_url = \"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={}&interval=5min&month={}&outputsize=full&apikey={}\".format(ticker, year_month, alpha_vantage_apikey)\n",
        "        r = requests.get(intraday_price_url)\n",
        "\n",
        "        try:\n",
        "            data = r.json()['Time Series (5min)']\n",
        "        except:\n",
        "            time.sleep(1) # required to not hit API limits\n",
        "            continue\n",
        "\n",
        "        for key in data:\n",
        "            document = data[key]\n",
        "            document['datetime'] = key\n",
        "            document['ticker'] = ticker\n",
        "\n",
        "            document['open'] = document['1. open']\n",
        "            document['high'] = document['2. high']\n",
        "            document['low'] = document['3. low']\n",
        "            document['close'] = document['4. close']\n",
        "            document['volume'] = document['5. volume']\n",
        "\n",
        "            document['open'] = float(document['open'])\n",
        "            document['high'] = float(document['high'])\n",
        "            document['low'] = float(document['low'])\n",
        "            document['close'] = float(document['close'])\n",
        "            document['volume'] = int(document['volume'])\n",
        "\n",
        "\n",
        "            del document['1. open']\n",
        "            del document['2. high']\n",
        "            del document['3. low']\n",
        "            del document['4. close']\n",
        "            del document['5. volume']\n",
        "\n",
        "            data_list += [document]\n",
        "\n",
        "            # Inside your loop, create the params dictionary with the correct values\n",
        "            params = {\n",
        "                'datetime': document['datetime'],\n",
        "                'ticker': ticker,\n",
        "                'open': document['open'],\n",
        "                'high': document['high'],\n",
        "                'low': document['low'],\n",
        "                'close': document['close'],\n",
        "                'volume': document['volume']\n",
        "            }\n",
        "\n",
        "            # Construct and execute the SQL statement\n",
        "            table_name = 'stockTable'\n",
        "            stmt = f\"INSERT INTO {table_name} (created_at, ticker, open, high, low, close, volume) VALUES (%(datetime)s, %(ticker)s, %(open)s, %(high)s, %(low)s, %(close)s, %(volume)s)\"\n",
        "\n",
        "            with s2_conn.cursor() as cur:\n",
        "                cur.execute(stmt, params)\n",
        "        # time.sleep(1) # required to not hit API limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n            <th>count(*)</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>13717</td>\n        </tr>\n    </tbody>\n</table>",
            "text/plain": "+----------+\n| count(*) |\n+----------+\n|  13717   |\n+----------+"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "select count(*) from stockTable"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "## Bring in Company data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "TSLA\n"
        },
        {
          "ename": "RuntimeError",
          "evalue": "{'Information': 'Thank you for using Alpha Vantage! Our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIK\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(data))\n\u001b[1;32m     24\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIK\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIK\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarketCapitalization\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m float_or_none(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarketCapitalization\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[0;31mRuntimeError\u001b[0m: {'Information': 'Thank you for using Alpha Vantage! Our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}"
          ]
        }
      ],
      "source": [
        "def float_or_none(x):\n",
        "    if x is None or x == 'None':\n",
        "        return None\n",
        "    return float(x)\n",
        "\n",
        "# pull intraday data for each stock and write to SingleStore\n",
        "for ticker in ticker_list:\n",
        "    print(ticker)\n",
        "    data_list = []\n",
        "    # for year_month in year_month_list:\n",
        "\n",
        "    company_overview = \"https://www.alphavantage.co/query?function=OVERVIEW&symbol={}&outputsize=full&apikey={}\".format(ticker, alpha_vantage_apikey)\n",
        "    r = requests.get(company_overview)\n",
        "\n",
        "    try:\n",
        "        data = r.json()\n",
        "    except:\n",
        "        time.sleep(3) # required to not hit API limits\n",
        "        continue\n",
        "\n",
        "    if 'CIK' not in data:\n",
        "        raise RuntimeError(str(data))\n",
        "\n",
        "    data['CIK'] = int(data['CIK'])\n",
        "    data['MarketCapitalization']= float_or_none(data['MarketCapitalization'])\n",
        "    # Assuming data['EBITDA'] is a string containing 'None'\n",
        "    ebitda_str = data['EBITDA']\n",
        "    if ebitda_str.lower() == 'none':\n",
        "        # Handle the case where EBITDA is 'None', for example, you can set it to 0\n",
        "        data['EBITDA'] = 0.0\n",
        "    else:\n",
        "        # Convert the EBITDA string to a float\n",
        "        data['EBITDA'] = float_or_none(ebitda_str)\n",
        "\n",
        "    PERatio_flt = data['PERatio']\n",
        "    if PERatio_flt.lower() == 'none':\n",
        "        # Handle the case where EVToRevenue is '-'\n",
        "        data['PERatio'] = 0.0  # You can use any default value that makes sense\n",
        "    else:\n",
        "        # Convert the EVToRevenue string to a float\n",
        "        data['PERatio'] = float_or_none(PERatio_flt)\n",
        "\n",
        "    data['PEGRatio']= float_or_none(data['PEGRatio'])\n",
        "    data['BookValue']= float_or_none(data['BookValue'])\n",
        "    data['DividendPerShare']= float_or_none(data['DividendPerShare'])\n",
        "    data['DividendYield']= float_or_none(data['DividendYield'])\n",
        "    data['EPS']= float_or_none(data['EPS'])\n",
        "    data['RevenuePerShareTTM']= float_or_none(data['RevenuePerShareTTM'])\n",
        "    data['ProfitMargin']= float_or_none(data['ProfitMargin'])\n",
        "    data['OperatingMarginTTM']= float_or_none(data['OperatingMarginTTM'])\n",
        "    data['ReturnOnAssetsTTM']= float_or_none(data['ReturnOnAssetsTTM'])\n",
        "    data['ReturnOnEquityTTM']= float_or_none(data['ReturnOnEquityTTM'])\n",
        "    data['RevenueTTM']= int(data['RevenueTTM'])\n",
        "    data['GrossProfitTTM']= int(data['GrossProfitTTM'])\n",
        "    data['DilutedEPSTTM']= float_or_none(data['DilutedEPSTTM'])\n",
        "    data['QuarterlyEarningsGrowthYOY']= float_or_none(data['QuarterlyEarningsGrowthYOY'])\n",
        "    data['QuarterlyRevenueGrowthYOY']= float_or_none(data['QuarterlyRevenueGrowthYOY'])\n",
        "    data['AnalystTargetPrice']= float_or_none(data['AnalystTargetPrice'])\n",
        "    # Assuming data['TrailingPE'] is a string containing '-'\n",
        "    trailing_pe_str = data['TrailingPE']\n",
        "    if trailing_pe_str == '-':\n",
        "        # Handle the case where TrailingPE is '-'\n",
        "        data['TrailingPE'] = 0.0  # You can use any default value that makes sense\n",
        "    else:\n",
        "        try:\n",
        "            # Attempt to convert the TrailingPE string to a float\n",
        "            data['TrailingPE'] = float_or_none(trailing_pe_str)\n",
        "        except ValueError:\n",
        "            # Handle the case where the conversion fails (e.g., if it contains invalid characters)\n",
        "            data['TrailingPE'] = 0.0  # Set to a default value or handle as needed\n",
        "\n",
        "    data['ForwardPE']= float_or_none(data['ForwardPE'])\n",
        "    data['PriceToSalesRatioTTM']= float_or_none(data['PriceToSalesRatioTTM'])\n",
        "    # Assuming data['EVToRevenue'] is a string containing '-'\n",
        "    PriceToBookRatio_flt = data['PriceToBookRatio']\n",
        "    if PriceToBookRatio_flt == '-':\n",
        "        # Handle the case where EVToRevenue is '-'\n",
        "        data['PriceToBookRatio'] = 0.0  # You can use any default value that makes sense\n",
        "    else:\n",
        "        # Convert the EVToRevenue string to a float\n",
        "        data['PriceToBookRatio'] = float_or_none(PriceToBookRatio_flt)\n",
        "\n",
        "    # Assuming data['EVToRevenue'] is a string containing '-'\n",
        "    ev_to_revenue_str = data['EVToRevenue']\n",
        "    if ev_to_revenue_str == '-':\n",
        "        # Handle the case where EVToRevenue is '-'\n",
        "        data['EVToRevenue'] = 0.0  # You can use any default value that makes sense\n",
        "    else:\n",
        "        # Convert the EVToRevenue string to a float\n",
        "        data['EVToRevenue'] = float_or_none(ev_to_revenue_str)\n",
        "\n",
        "    # data['EVToEBITDA']= float(data['EVToEBITDA'])\n",
        "    # Assuming data['EVToRevenue'] is a string containing '-'\n",
        "    ev_to_EBITDA_str = data['EVToEBITDA']\n",
        "    if ev_to_revenue_str == '-':\n",
        "        # Handle the case where EVToRevenue is '-'\n",
        "        data['EVToEBITDA'] = 0.0  # You can use any default value that makes sense\n",
        "    else:\n",
        "        # Convert the EVToRevenue string to a float\n",
        "        data['EVToEBITDA'] = float_or_none(ev_to_EBITDA_str)\n",
        "\n",
        "    data['Beta']= float_or_none(data['Beta'])\n",
        "    data['52WeekHigh']= float_or_none(data['52WeekHigh'])\n",
        "    data['52WeekLow']= float_or_none(data['52WeekLow'])\n",
        "    data['50DayMovingAverage']= float_or_none(data['50DayMovingAverage'])\n",
        "    data['200DayMovingAverage']= float_or_none(data['200DayMovingAverage'])\n",
        "    data['SharesOutstanding']= int(data['SharesOutstanding'])\n",
        "    # description_embedding = [np.array(x, '<f4') for x in get_embeddings(data[\"Description\"], engine=model)]\n",
        "    dividend_date_str = data['DividendDate']\n",
        "    if dividend_date_str.lower() == 'none':\n",
        "        # Handle the case where EBITDA is 'None', for example, you can set it to 0\n",
        "        data['DividendDate'] = '9999-12-31'\n",
        "    else:\n",
        "        # Convert the EBITDA string to a float\n",
        "        data['DividendDate'] = str(dividend_date_str)\n",
        "\n",
        "    exdividend_date_str = data['ExDividendDate']\n",
        "    if exdividend_date_str.lower() == 'none':\n",
        "        # Handle the case where EBITDA is 'None', for example, you can set it to 0\n",
        "        data['ExDividendDate'] = '9999-12-31'\n",
        "    else:\n",
        "        # Convert the EBITDA string to a float\n",
        "        data['ExDividendDate'] = str(exdividend_date_str)\n",
        "\n",
        "    data_list += [data]\n",
        "\n",
        "    # Inside your loop, create the params dictionary with the correct values\n",
        "    params = {\n",
        "                \"Symbol\": data[\"Symbol\"],\n",
        "                \"AssetType\": data[\"AssetType\"],\n",
        "                \"Name\": data[\"Name\"],\n",
        "                \"Description\": data[\"Description\"],\n",
        "                \"CIK\": data[\"CIK\"],\n",
        "                \"Exchange\": data[\"Exchange\"],\n",
        "                \"Currency\": data[\"Currency\"],\n",
        "                \"Country\": data[\"Country\"],\n",
        "                \"Sector\": data[\"Sector\"],\n",
        "                \"Industry\": data[\"Industry\"],\n",
        "                \"Address\": data[\"Address\"],\n",
        "                \"FiscalYearEnd\": data[\"FiscalYearEnd\"],\n",
        "                \"LatestQuarter\": data[\"LatestQuarter\"],\n",
        "                \"MarketCapitalization\": data[\"MarketCapitalization\"],\n",
        "                \"EBITDA\": data[\"EBITDA\"],\n",
        "                \"PERatio\": data[\"PERatio\"],\n",
        "                \"PEGRatio\": data[\"PEGRatio\"],\n",
        "                \"BookValue\": data[\"BookValue\"],\n",
        "                \"DividendPerShare\": data[\"DividendPerShare\"],\n",
        "                \"DividendYield\": data[\"DividendYield\"],\n",
        "                \"EPS\": data[\"EPS\"],\n",
        "                \"RevenuePerShareTTM\": data[\"RevenuePerShareTTM\"],\n",
        "                \"ProfitMargin\": data[\"ProfitMargin\"],\n",
        "                \"OperatingMarginTTM\": data[\"OperatingMarginTTM\"],\n",
        "                \"ReturnOnAssetsTTM\": data[\"ReturnOnAssetsTTM\"],\n",
        "                \"ReturnOnEquityTTM\": data[\"ReturnOnEquityTTM\"],\n",
        "                \"RevenueTTM\": data[\"RevenueTTM\"],\n",
        "                \"GrossProfitTTM\": data[\"GrossProfitTTM\"],\n",
        "                \"DilutedEPSTTM\": data[\"DilutedEPSTTM\"],\n",
        "                \"QuarterlyEarningsGrowthYOY\": data[\"QuarterlyEarningsGrowthYOY\"],\n",
        "                \"QuarterlyRevenueGrowthYOY\": data[\"QuarterlyRevenueGrowthYOY\"],\n",
        "                \"AnalystTargetPrice\": data[\"AnalystTargetPrice\"],\n",
        "                \"TrailingPE\": data[\"TrailingPE\"],\n",
        "                \"ForwardPE\": data[\"ForwardPE\"],\n",
        "                \"PriceToSalesRatioTTM\": data[\"PriceToSalesRatioTTM\"],\n",
        "                \"PriceToBookRatio\": data[\"PriceToBookRatio\"],\n",
        "                \"EVToRevenue\": data[\"EVToRevenue\"],\n",
        "                \"EVToEBITDA\": data[\"EVToEBITDA\"],\n",
        "                \"Beta\": data[\"Beta\"],\n",
        "                \"52WeekHigh\": data[\"52WeekHigh\"],\n",
        "                \"52WeekLow\": data[\"52WeekLow\"],\n",
        "                \"50DayMovingAverage\": data[\"50DayMovingAverage\"],\n",
        "                \"200DayMovingAverage\": data[\"200DayMovingAverage\"],\n",
        "                \"SharesOutstanding\": data[\"SharesOutstanding\"],\n",
        "                \"DividendDate\": data[\"DividendDate\"],\n",
        "                \"ExDividendDate\": data[\"ExDividendDate\"]\n",
        "    }\n",
        "\n",
        "     # Construct and execute the SQL statement\n",
        "    table_name = 'companyInfo'\n",
        "    stmt = f\"INSERT INTO {table_name} (ticker, AssetType, Name, Description, CIK, Exchange, Currency, Country, Sector, Industry, Address, FiscalYearEnd, LatestQuarter, MarketCapitalization, EBITDA, PERatio, PEGRatio, BookValue, DividendPerShare, DividendYield, EPS, RevenuePerShareTTM, ProfitMargin, OperatingMarginTTM, ReturnOnAssetsTTM, ReturnOnEquityTTM, RevenueTTM, GrossProfitTTM, DilutedEPSTTM, QuarterlyEarningsGrowthYOY, QuarterlyRevenueGrowthYOY, AnalystTargetPrice, TrailingPE, ForwardPE, PriceToSalesRatioTTM, PriceToBookRatio, EVToRevenue, EVToEBITDA, Beta, 52WeekHigh, 52WeekLow, 50DayMovingAverage, 200DayMovingAverage, SharesOutstanding, DividendDate, ExDividendDate) VALUES (%(Symbol)s, %(AssetType)s, %(Name)s, %(Description)s, %(CIK)s, %(Exchange)s, %(Currency)s, %(Country)s, %(Sector)s, %(Industry)s, %(Address)s, %(FiscalYearEnd)s, %(LatestQuarter)s, %(MarketCapitalization)s, %(EBITDA)s, %(PERatio)s, %(PEGRatio)s, %(BookValue)s, %(DividendPerShare)s, %(DividendYield)s, %(EPS)s, %(RevenuePerShareTTM)s, %(ProfitMargin)s, %(OperatingMarginTTM)s, %(ReturnOnAssetsTTM)s, %(ReturnOnEquityTTM)s, %(RevenueTTM)s, %(GrossProfitTTM)s, %(DilutedEPSTTM)s, %(QuarterlyEarningsGrowthYOY)s, %(QuarterlyRevenueGrowthYOY)s, %(AnalystTargetPrice)s, %(TrailingPE)s, %(ForwardPE)s, %(PriceToSalesRatioTTM)s, %(PriceToBookRatio)s, %(EVToRevenue)s, %(EVToEBITDA)s, %(Beta)s, %(52WeekHigh)s, %(52WeekLow)s, %(50DayMovingAverage)s, %(200DayMovingAverage)s, %(SharesOutstanding)s, %(DividendDate)s, %(ExDividendDate)s)\"\n",
        "\n",
        "# Replace table_name with the actual table name you're using.\n",
        "    with s2_conn.cursor() as cur:\n",
        "        cur.execute(stmt, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n            <th>ticker</th>\n            <th>AssetType</th>\n            <th>Name</th>\n            <th>Description</th>\n            <th>CIK</th>\n            <th>Exchange</th>\n            <th>Currency</th>\n            <th>Country</th>\n            <th>Sector</th>\n            <th>Industry</th>\n            <th>Address</th>\n            <th>FiscalYearEnd</th>\n            <th>LatestQuarter</th>\n            <th>MarketCapitalization</th>\n            <th>EBITDA</th>\n            <th>PERatio</th>\n            <th>PEGRatio</th>\n            <th>BookValue</th>\n            <th>DividendPerShare</th>\n            <th>DividendYield</th>\n            <th>EPS</th>\n            <th>RevenuePerShareTTM</th>\n            <th>ProfitMargin</th>\n            <th>OperatingMarginTTM</th>\n            <th>ReturnOnAssetsTTM</th>\n            <th>ReturnOnEquityTTM</th>\n            <th>RevenueTTM</th>\n            <th>GrossProfitTTM</th>\n            <th>DilutedEPSTTM</th>\n            <th>QuarterlyEarningsGrowthYOY</th>\n            <th>QuarterlyRevenueGrowthYOY</th>\n            <th>AnalystTargetPrice</th>\n            <th>TrailingPE</th>\n            <th>ForwardPE</th>\n            <th>PriceToSalesRatioTTM</th>\n            <th>PriceToBookRatio</th>\n            <th>EVToRevenue</th>\n            <th>EVToEBITDA</th>\n            <th>Beta</th>\n            <th>52WeekHigh</th>\n            <th>52WeekLow</th>\n            <th>50DayMovingAverage</th>\n            <th>200DayMovingAverage</th>\n            <th>SharesOutstanding</th>\n            <th>DividendDate</th>\n            <th>ExDividendDate</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>AMZN</td>\n            <td>Common Stock</td>\n            <td>Amazon.com Inc</td>\n            <td>Amazon.com, Inc. is an American multinational technology company which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is one of the Big Five companies in the U.S. information technology industry, along with Google, Apple, Microsoft, and Facebook. The company has been referred to as one of the most influential economic and cultural forces in the world, as well as the world's most valuable brand.</td>\n            <td>1018724</td>\n            <td>NASDAQ</td>\n            <td>USD</td>\n            <td>USA</td>\n            <td>TRADE & SERVICES</td>\n            <td>RETAIL-CATALOG & MAIL-ORDER HOUSES</td>\n            <td>410 TERRY AVENUE NORTH, SEATTLE, WA, US</td>\n            <td>December</td>\n            <td>2024-03-31</td>\n            <td>1762211135000</td>\n            <td>104049000000</td>\n            <td>39.98</td>\n            <td>1.784</td>\n            <td>22.54</td>\n            <td>None</td>\n            <td>None</td>\n            <td>4.20</td>\n            <td>58.22</td>\n            <td>0.0735</td>\n            <td>0.0992</td>\n            <td>0.0658</td>\n            <td>0.2190</td>\n            <td>604333998000</td>\n            <td>225152000000</td>\n            <td>4.20</td>\n            <td>0.938</td>\n            <td>0.101</td>\n            <td>223.28</td>\n            <td>39.98</td>\n            <td>35.97</td>\n            <td>2.916</td>\n            <td>7.45</td>\n            <td>2.987</td>\n            <td>16.95</td>\n            <td>1.155</td>\n            <td>201.20</td>\n            <td>118.35</td>\n            <td>186.80</td>\n            <td>168.53</td>\n            <td>10495600000</td>\n            <td>9999-12-31</td>\n            <td>9999-12-31</td>\n        </tr>\n    </tbody>\n</table>",
            "text/plain": "+--------+--------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+----------+---------+------------------+------------------------------------+-----------------------------------------+---------------+---------------+----------------------+--------------+---------+----------+-----------+------------------+---------------+------+--------------------+--------------+--------------------+-------------------+-------------------+--------------+----------------+---------------+----------------------------+---------------------------+--------------------+------------+-----------+----------------------+------------------+-------------+------------+-------+------------+-----------+--------------------+---------------------+-------------------+--------------+----------------+\n| ticker |  AssetType   |      Name      |                                                                                                                                                                                                                        Description                                                                                                                                                                                                                        |   CIK   | Exchange | Currency | Country |      Sector      |              Industry              |                 Address                 | FiscalYearEnd | LatestQuarter | MarketCapitalization |    EBITDA    | PERatio | PEGRatio | BookValue | DividendPerShare | DividendYield | EPS  | RevenuePerShareTTM | ProfitMargin | OperatingMarginTTM | ReturnOnAssetsTTM | ReturnOnEquityTTM |  RevenueTTM  | GrossProfitTTM | DilutedEPSTTM | QuarterlyEarningsGrowthYOY | QuarterlyRevenueGrowthYOY | AnalystTargetPrice | TrailingPE | ForwardPE | PriceToSalesRatioTTM | PriceToBookRatio | EVToRevenue | EVToEBITDA |  Beta | 52WeekHigh | 52WeekLow | 50DayMovingAverage | 200DayMovingAverage | SharesOutstanding | DividendDate | ExDividendDate |\n+--------+--------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+----------+---------+------------------+------------------------------------+-----------------------------------------+---------------+---------------+----------------------+--------------+---------+----------+-----------+------------------+---------------+------+--------------------+--------------+--------------------+-------------------+-------------------+--------------+----------------+---------------+----------------------------+---------------------------+--------------------+------------+-----------+----------------------+------------------+-------------+------------+-------+------------+-----------+--------------------+---------------------+-------------------+--------------+----------------+\n|  AMZN  | Common Stock | Amazon.com Inc | Amazon.com, Inc. is an American multinational technology company which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is one of the Big Five companies in the U.S. information technology industry, along with Google, Apple, Microsoft, and Facebook. The company has been referred to as one of the most influential economic and cultural forces in the world, as well as the world's most valuable brand. | 1018724 |  NASDAQ  |   USD    |   USA   | TRADE & SERVICES | RETAIL-CATALOG & MAIL-ORDER HOUSES | 410 TERRY AVENUE NORTH, SEATTLE, WA, US |    December   |   2024-03-31  |    1762211135000     | 104049000000 |  39.98  |  1.784   |   22.54   |       None       |      None     | 4.20 |       58.22        |    0.0735    |       0.0992       |       0.0658      |       0.2190      | 604333998000 |  225152000000  |      4.20     |           0.938            |           0.101           |       223.28       |   39.98    |   35.97   |        2.916         |       7.45       |    2.987    |   16.95    | 1.155 |   201.20   |   118.35  |       186.80       |        168.53       |    10495600000    |  9999-12-31  |   9999-12-31   |\n+--------+--------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+----------+---------+------------------+------------------------------------+-----------------------------------------+---------------+---------------+----------------------+--------------+---------+----------+-----------+------------------+---------------+------+--------------------+--------------+--------------------+-------------------+-------------------+--------------+----------------+---------------+----------------------------+---------------------------+--------------------+------------+-----------+----------------------+------------------+-------------+------------+-------+------------+-----------+--------------------+---------------------+-------------------+--------------+----------------+"
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "select * from companyInfo limit 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "## Bring in news sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "TSLA\n2024-08-01 00:00:00\n2024-07-01 00:00:00\nAMZN\n2024-08-01 00:00:00\n2024-07-01 00:00:00\nPLTR\n2024-08-01 00:00:00\n2024-07-01 00:00:00\n"
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "# pull intraday data for each stock and write to Mongo\n",
        "for ticker in ticker_list:\n",
        "    print(ticker)\n",
        "    data_list = []\n",
        "\n",
        "    for i in year_month_list:\n",
        "        date_object = datetime.datetime.strptime(i, '%Y-%m')\n",
        "        print(date_object)\n",
        "        output_date = date_object.strftime('%Y%m%d') + \"T0000\"\n",
        "\n",
        "        # Get the next month from the 'date_object'\n",
        "        previous_month_date = date_object + relativedelta(months=-1)\n",
        "        previous_month_date = previous_month_date.strftime('%Y%m%d') + \"T0000\"\n",
        "\n",
        "        # Update 'date_object' for the next iteration\n",
        "        date_object = previous_month_date\n",
        "\n",
        "        # replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
        "        news_and_sentiment = 'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={}&time_from={}&time_to={}&limit=1000&outputsize=full&apikey={}'.format(ticker, previous_month_date, output_date, alpha_vantage_apikey)\n",
        "        r = requests.get(news_and_sentiment)\n",
        "\n",
        "        try:\n",
        "            data = r.json()\n",
        "            data = data[\"feed\"]\n",
        "        except:\n",
        "            time.sleep(2) # required to not hit API limits\n",
        "            continue\n",
        "\n",
        "        for item in data:\n",
        "            item['title'] = str(item['title'])\n",
        "            item['url'] = str(item['url'])\n",
        "            item['time_published'] = datetime.datetime.strptime(str(item['time_published']), \"%Y%m%dT%H%M%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            if item['authors']:\n",
        "                # Check if the 'authors' list is not empty\n",
        "                authors_str = str(item['authors'][0])\n",
        "            else:\n",
        "                # Handle the case where 'authors' is empty\n",
        "                authors_str = \"No authors available\"\n",
        "\n",
        "            item['authors'] = authors_str\n",
        "\n",
        "            item['summary'] = str(item['summary'])\n",
        "            item['banner_image'] = str(item['banner_image'])\n",
        "            item['source'] = str(item['source'])\n",
        "            item['category_within_source'] = str(item['category_within_source'])\n",
        "            item['source_domain'] = str(item['source_domain'])\n",
        "            item['topic'] = str(item['topics'][0][\"topic\"])\n",
        "            item['topic_relevance_score'] = float(item['topics'][0]['relevance_score'])\n",
        "            item['overall_sentiment_score'] = float(item['overall_sentiment_score'])\n",
        "            item['overall_sentiment_label'] = str(item['overall_sentiment_label'])\n",
        "            item['ticker'] = str(item['ticker_sentiment'][0]['ticker'])\n",
        "            item['ticker_relevance_score'] = float(item['ticker_sentiment'][0]['relevance_score'])\n",
        "            item['ticker_sentiment_score'] = float(item['ticker_sentiment'][0]['ticker_sentiment_score'])\n",
        "            item['ticker_sentiment_label'] = str(item['ticker_sentiment'][0]['ticker_sentiment_label'])\n",
        "\n",
        "            params= {\n",
        "                \"title\": item[\"title\"],\n",
        "                \"url\": item[\"url\"],\n",
        "                \"time_published\": item[\"time_published\"],\n",
        "                \"authors\": item[\"authors\"],\n",
        "                \"summary\": item[\"summary\"],\n",
        "                \"banner_image\": item[\"banner_image\"],\n",
        "                \"source\": item[\"source\"],\n",
        "                \"category_within_source\": item[\"category_within_source\"],\n",
        "                \"source_domain\": item[\"source_domain\"],\n",
        "                \"topic\": item[\"topic\"],\n",
        "                \"topic_relevance_score\": item['topic_relevance_score'],\n",
        "                'overall_sentiment_score': item['overall_sentiment_score'],\n",
        "                'overall_sentiment_label': item['overall_sentiment_label'],\n",
        "                'ticker': item['ticker'],\n",
        "                'ticker_relevance_score': item['ticker_relevance_score'],\n",
        "                'ticker_sentiment_score': item['ticker_sentiment_score'],\n",
        "                'ticker_sentiment_label': item['ticker_sentiment_label']\n",
        "            }\n",
        "            #print(params)\n",
        "\n",
        "            # Construct and execute the SQL statement\n",
        "            table_name = 'newsSentiment'\n",
        "            stmt = f\"INSERT INTO {table_name} (title, url, time_published, authors, summary, banner_image, source, category_within_source, source_domain, topic, topic_relevance_score, overall_sentiment_score, overall_sentiment_label, ticker, ticker_relevance_score, ticker_sentiment_score, ticker_sentiment_label) VALUES (%(title)s, %(url)s, %(time_published)s, %(authors)s, %(summary)s, %(banner_image)s, %(source)s, %(category_within_source)s, %(source_domain)s, %(topic)s, %(topic_relevance_score)s, %(overall_sentiment_score)s, %(overall_sentiment_label)s, %(ticker)s, %(ticker_relevance_score)s, %(ticker_sentiment_score)s, %(ticker_sentiment_label)s)\"\n",
        "\n",
        "            # Replace table_name with the actual table name you're using.\n",
        "\n",
        "            with s2_conn.cursor() as cur:\n",
        "                cur.execute(stmt, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n            <th>Rows_in_newsSentiment</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>2359</td>\n        </tr>\n    </tbody>\n</table>",
            "text/plain": "+-----------------------+\n| Rows_in_newsSentiment |\n+-----------------------+\n|          2359         |\n+-----------------------+"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "SELECT count(*) Rows_in_newsSentiment FROM newsSentiment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"connect_s2\"></a>\n",
        "- [Back to Demo Architecture](#architecture)\n",
        "## Connect SingleStore to Open AI's LLM with Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n  warn_deprecated(\n"
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'create_tool_calling_agent' from 'langchain.agents' (/opt/conda/lib/python3.11/site-packages/langchain/agents/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(openai_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m], temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m toolkit \u001b[38;5;241m=\u001b[39m SQLDatabaseToolkit(db\u001b[38;5;241m=\u001b[39mdb, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m---> 10\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sql_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoolkit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoolkit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m    You are an agent designed to interact with a SQL database called SingleStore. This sometimes has Shard and Sort keys in the table schemas, which you can ignore.\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mGiven an input question, create a syntactically correct MySQL query to run, then look at the results of the query and return the answer.\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m If you are asked about similarity questions, you should use the DOT_PRODUCT function.\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mHere are a few examples of how to use the DOT_PRODUCT function:\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mExample 1:\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m    Q: how similar are the questions and answers?\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m    A: The query used to find this is:\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m        select question, answer, dot_product(question_embedding, answer_embedding) as similarity from embeddings;\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mExample 2:\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43m    Q: What are the most similar questions in the embeddings table, not including itself?\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m    A: The query used to find this answer is:\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43m        SELECT q1.question as question1, q2.question as question2, DOT_PRODUCT(q1.question_embedding, q2.question_embedding) :> float as score\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m        FROM embeddings q1, embeddings q2\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;43m        WHERE question1 != question2\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43m        ORDER BY score DESC LIMIT 5;\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mExample 3:\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43m    Q: In the embeddings table, which rows are from the chatbot?\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m    A: The query used to find this answer is:\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m        SELECT category, question, answer FROM embeddings\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;43m        WHERE category = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchatbot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mIf you are asked to describe the database, you should run the query SHOW TABLES\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most \u001b[39;49m\u001b[38;5;132;43;01m{top_k}\u001b[39;49;00m\u001b[38;5;124;43m results.\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m The question embeddings and answer embeddings are very long, so do not show them unless specifically asked to.\u001b[39;49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou can order the results by a relevant column to return the most interesting examples in the database.\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNever query for all the columns from a specific table, only ask for the relevant columns given the question.\u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou have access to tools for interacting with the database.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mOnly use the below tools.\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;43m    Only use the information returned by the below tools to construct your final answer.\u001b[39;49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again up to 3 times.\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mIf the question does not seem related to the database, just return \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI don\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mt know\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m as the answer.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformat_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[38;5;124;43mUse the following format:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mQuestion: the input question you must answer\u001b[39;49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mThought: you should always think about what to do\u001b[39;49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAction: the action to take, should be one of [\u001b[39;49m\u001b[38;5;132;43;01m{tool_names}\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAction Input: the input to the action\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mObservation: the result of the action\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mThought: I now know the final answer\u001b[39;49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mFinal Answer: the final answer to the original input question\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSQL Query used to get the Answer: the final sql query used for the final answer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_community/agent_toolkits/sql/base.py:117\u001b[0m, in \u001b[0;36mcreate_sql_agent\u001b[0;34m(llm, toolkit, agent_type, callback_manager, prefix, suffix, format_instructions, input_variables, top_k, max_iterations, max_execution_time, early_stopping_method, verbose, agent_executor_kwargs, extra_tools, db, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_sql_agent\u001b[39m(\n\u001b[1;32m     45\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     46\u001b[0m     toolkit: Optional[SQLDatabaseToolkit] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     66\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentExecutor:\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct a SQL agent from an LLM and toolkit or database.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    118\u001b[0m         create_openai_functions_agent,\n\u001b[1;32m    119\u001b[0m         create_openai_tools_agent,\n\u001b[1;32m    120\u001b[0m         create_react_agent,\n\u001b[1;32m    121\u001b[0m         create_tool_calling_agent,\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    124\u001b[0m         AgentExecutor,\n\u001b[1;32m    125\u001b[0m         RunnableAgent,\n\u001b[1;32m    126\u001b[0m         RunnableMultiActionAgent,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentType\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_tool_calling_agent' from 'langchain.agents' (/opt/conda/lib/python3.11/site-packages/langchain/agents/__init__.py)"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = openai_apikey\n",
        "embedding_model = 'text-embedding-ada-002'\n",
        "gpt_model = 'gpt-3.5-turbo-16k'\n",
        "\n",
        "# Create the agent executor\n",
        "db = SQLDatabase.from_uri(connection_url, include_tables=['embeddings', 'companyInfo', 'newsSentiment', 'stockTable'], sample_rows_in_table_info=2)\n",
        "llm = OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], temperature=0, verbose=True)\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "agent_executor = create_sql_agent(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    toolkit=toolkit,\n",
        "    verbose=True,\n",
        "    prefix= '''\n",
        "    You are an agent designed to interact with a SQL database called SingleStore. This sometimes has Shard and Sort keys in the table schemas, which you can ignore.\n",
        "    \\nGiven an input question, create a syntactically correct MySQL query to run, then look at the results of the query and return the answer.\n",
        "    \\n If you are asked about similarity questions, you should use the DOT_PRODUCT function.\n",
        "\n",
        "    \\nHere are a few examples of how to use the DOT_PRODUCT function:\n",
        "    \\nExample 1:\n",
        "    Q: how similar are the questions and answers?\n",
        "    A: The query used to find this is:\n",
        "\n",
        "        select question, answer, dot_product(question_embedding, answer_embedding) as similarity from embeddings;\n",
        "\n",
        "    \\nExample 2:\n",
        "    Q: What are the most similar questions in the embeddings table, not including itself?\n",
        "    A: The query used to find this answer is:\n",
        "\n",
        "        SELECT q1.question as question1, q2.question as question2, DOT_PRODUCT(q1.question_embedding, q2.question_embedding) :> float as score\n",
        "        FROM embeddings q1, embeddings q2\n",
        "        WHERE question1 != question2\n",
        "        ORDER BY score DESC LIMIT 5;\n",
        "\n",
        "    \\nExample 3:\n",
        "    Q: In the embeddings table, which rows are from the chatbot?\n",
        "    A: The query used to find this answer is:\n",
        "\n",
        "        SELECT category, question, answer FROM embeddings\n",
        "        WHERE category = 'chatbot';\n",
        "\n",
        "    \\nIf you are asked to describe the database, you should run the query SHOW TABLES\n",
        "    \\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\n",
        "    \\n The question embeddings and answer embeddings are very long, so do not show them unless specifically asked to.\n",
        "    \\nYou can order the results by a relevant column to return the most interesting examples in the database.\n",
        "    \\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
        "    \\nYou have access to tools for interacting with the database.\\nOnly use the below tools.\n",
        "    Only use the information returned by the below tools to construct your final answer.\n",
        "    \\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again up to 3 times.\n",
        "    \\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
        "    \\n\\nIf the question does not seem related to the database, just return \"I don\\'t know\" as the answer.\\n,\n",
        "\n",
        "    ''',\n",
        "    format_instructions='''Use the following format:\\n\n",
        "    \\nQuestion: the input question you must answer\n",
        "    \\nThought: you should always think about what to do\n",
        "    \\nAction: the action to take, should be one of [{tool_names}]\n",
        "    \\nAction Input: the input to the action\n",
        "    \\nObservation: the result of the action\n",
        "    \\nThought: I now know the final answer\n",
        "    \\nFinal Answer: the final answer to the original input question\n",
        "    \\nSQL Query used to get the Answer: the final sql query used for the final answer'\n",
        "    ''',\n",
        "    top_k=3,\n",
        "    max_iterations=5\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Create function that processes user question with a check in Semantic Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_name = 'embeddings'\n",
        "similarity_threshold = .97\n",
        "\n",
        "def process_user_question(question):\n",
        "    print(f'\\nQuestion asked: {question}')\n",
        "    category = 'chatbot'\n",
        "\n",
        "    # Get vector embedding from the original question and calculate the elapsed time\n",
        "    start_time = time.time()\n",
        "    question_embedding= [np.array(x, '<f4') for x in get_embeddings([question], api_key=openai_apikey, engine=embedding_model)]\n",
        "    elapsed_time = (time.time() - start_time) * 1000\n",
        "    print(f\"Execution time for getting the question embedding: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "    params = {\n",
        "              'question_embedding': question_embedding,\n",
        "            }\n",
        "\n",
        "    # Check if embedding is similar to existing questions\n",
        "    # If semantic score < similarity_threshold, then run the agent executor\n",
        "    # Calculate elapsed time for this step\n",
        "\n",
        "    stmt = f'select question, answer, dot_product( %(question_embedding)s, question_embedding) :> float as score from embeddings where category=\"chatbot\" order by score desc limit 1;'\n",
        "\n",
        "\n",
        "    with s2_conn.cursor() as cur:\n",
        "        start_time = time.time()\n",
        "        cur.execute(stmt, params)\n",
        "        row = cur.fetchone()\n",
        "        elapsed_time = (time.time() - start_time) * 1000\n",
        "        print(f\"Execution time for checking existing questions: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "        try:\n",
        "            question2, answer, score = row\n",
        "            print(f\"\\nClosest Matching row:\\nQuestion: {question2}\\nAnswer: {answer}\\nSimilarity Score: {score}\")\n",
        "\n",
        "            if score > similarity_threshold:\n",
        "                print('Action to take: Using existing answer')\n",
        "                return answer\n",
        "\n",
        "            else:\n",
        "                print('Action to take: Running agent_executor')\n",
        "                start_time = time.time()\n",
        "                answer2 = agent_executor.run(question)\n",
        "                elapsed_time = (time.time() - start_time) * 1000\n",
        "                print(f\"agent_executor execution time: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "                # Get current time\n",
        "                created_at = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "                # Get the answer embedding and calculate the elapsed time\n",
        "                start_time = time.time()\n",
        "                answer_embedding = [np.array(x, '<f4') for x in get_embeddings([answer2], api_key=openai_apikey, engine=embedding_model)]\n",
        "                elapsed_time = (time.time() - start_time) * 1000\n",
        "                print(f\"Answer embeddings execution time: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "                params = {'category': category, 'question': question,\n",
        "                        'question_embedding': question_embedding,\n",
        "                        'answer': answer2, 'answer_embedding': answer_embedding,\n",
        "                        'created_at': created_at}\n",
        "\n",
        "                # Send params details as a row into the SingleStoreDB embeddings table and calculate the elapsed time\n",
        "                stmt = f\"INSERT INTO {table_name} (category, question, question_embedding, answer, answer_embedding, created_at) VALUES (%(category)s, \\n%(question)s, \\n%(question_embedding)s, \\n%(answer)s, \\n%(answer_embedding)s, \\n%(created_at)s)\"\n",
        "                start_time = time.time()\n",
        "\n",
        "                with s2_conn.cursor() as cur:\n",
        "                    cur.execute(stmt, params)\n",
        "\n",
        "                elapsed_time = (time.time() - start_time) * 1000\n",
        "                print(f\"Insert to SingleStore execution time: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "                return answer2\n",
        "\n",
        "        # Handle known exceptions then run as normal\n",
        "        except:\n",
        "            print('No existing rows.  Running agent_executor')\n",
        "            start_time = time.time()\n",
        "            answer2 = agent_executor.run(question)\n",
        "            elapsed_time = (time.time() - start_time) * 1000\n",
        "            print(f\"agent_executor execution time: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "            created_at = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # Record the start time\n",
        "            start_time = time.time()\n",
        "\n",
        "            answer_embedding = [np.array(x, '<f4') for x in get_embeddings([answer2], api_key=openai_apikey, engine=embedding_model)]\n",
        "\n",
        "            # Calculate the elapsed time\n",
        "            elapsed_time = (time.time() - start_time) * 1000\n",
        "            print(f\"Answer embeddings execution time: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "            params = {'category': category, 'question': question,\n",
        "                    'question_embedding': question_embedding,\n",
        "                    'answer': answer2, 'answer_embedding': answer_embedding,\n",
        "                    'created_at': created_at}\n",
        "\n",
        "            # Send to SingleStoreDB\n",
        "            stmt = f\"INSERT INTO {table_name} (category, question, question_embedding, answer, answer_embedding, created_at) VALUES (%(category)s, \\n%(question)s, \\n%(question_embedding)s, \\n%(answer)s, \\n%(answer_embedding)s, \\n%(created_at)s)\"\n",
        "\n",
        "            # Record the start time\n",
        "            start_time = time.time()\n",
        "\n",
        "            with s2_conn.cursor() as cur:\n",
        "                cur.execute(stmt, params)\n",
        "\n",
        "            # Calculate the elapsed time\n",
        "            elapsed_time = (time.time() - start_time) * 1000\n",
        "            print(f\"Insert to SingleStore execution time: {elapsed_time:.2f} milliseconds\")\n",
        "\n",
        "            return answer2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Test on two similar questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "# Two similar questions\n",
        "question_1 = \"describe the database\"\n",
        "question_2 = \"describe database\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question: describe the database\n",
        "answer = process_user_question(question_1)\n",
        "print(f'The answer is: {answer}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "select id, category, question, answer from embeddings limit 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question: describe database\n",
        "answer = process_user_question(question_2)\n",
        "print(f'The answer is: {answer}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"speech\"></a>\n",
        "- [Back to Contents](#contents)\n",
        "## Add Voice Recognition and Speech"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Select a voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "from elevenlabs import generate, stream, voices\n",
        "from elevenlabs import set_api_key\n",
        "from IPython.display import Audio\n",
        "from IPython.display import display\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "voices = voices()\n",
        "voices[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "CHUNK_SIZE = 1024\n",
        "url = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM/stream\"\n",
        "\n",
        "headers = {\n",
        "  \"Accept\": \"audio/mpeg\",\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"xi-api-key\": elevenlabs_apikey\n",
        "}\n",
        "\n",
        "data = {\n",
        "  \"text\": answer,\n",
        "  \"model_id\": \"eleven_monolingual_v1\",\n",
        "  \"voice_settings\": {\n",
        "    \"stability\": 0.5,\n",
        "    \"similarity_boost\": 0.5\n",
        "  }\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data, headers=headers, stream=True)\n",
        "\n",
        "# create an audio file\n",
        "with open('output.mp3', 'wb') as f:\n",
        "    for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
        "        if chunk:\n",
        "            f.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_file = 'output.mp3'\n",
        "\n",
        "audio = Audio(filename=audio_file, autoplay =True)\n",
        "display(audio)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Transcribe the audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = openai_apikey\n",
        "audio_file= open(\"output.mp3\", \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "print(transcript[\"text\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"image\"></a>\n",
        "- [Back to Demo Architecture](#architecture)\n",
        "## Tying it together with Image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Most recent news article for TSLA\n",
        "question_3 = \"\"\"What is the most recent news article for Amazon where the topic_relevance_score is greater than 90%?\n",
        "Include the url, time published and banner image.\"\"\"\n",
        "answer = process_user_question(question_3)\n",
        "print(f'The answer is: {answer}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT title, url, time_published, banner_image FROM newsSentiment WHERE ticker = 'AMZN' AND topic_relevance_score > 0.9 ORDER BY time_published DESC LIMIT 3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Load the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from io import BytesIO\n",
        "banner_image_url = \"https://staticx-tuner.zacks.com/images/default_article_images/default341.jpg\"\n",
        "response = requests.get(banner_image_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    img = mpimg.imread(BytesIO(response.content), format='JPG')\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Failed to retrieve the image. Status code: {response.status_code}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "### Set up the huggingface transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformers_version = \"v4.29.0\" #@param [\"main\", \"v4.29.0\"] {allow-input: true}\n",
        "\n",
        "print(f\"Setting up everything with transformers version {transformers_version}\")\n",
        "\n",
        "!pip install huggingface_hub>=0.14.1 git+https://github.com/huggingface/transformers@$transformers_version -q diffusers accelerate datasets torch soundfile sentencepiece opencv-python openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython\n",
        "import soundfile as sf\n",
        "\n",
        "def play_audio(audio):\n",
        "    sf.write(\"speech_converted.wav\", audio.numpy(), samplerate=16000)\n",
        "    return IPython.display.Audio(\"speech_converted.wav\")\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_name = \"OpenAI (API Key)\" #@param [\"StarCoder (HF Token)\", \"OpenAssistant (HF Token)\", \"OpenAI (API Key)\"]\n",
        "\n",
        "if agent_name == \"StarCoder (HF Token)\":\n",
        "    from transformers.tools import HfAgent\n",
        "    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n",
        "    print(\"StarCoder is initialized \ud83d\udcaa\")\n",
        "elif agent_name == \"OpenAssistant (HF Token)\":\n",
        "    from transformers.tools import HfAgent\n",
        "    agent = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n",
        "    print(\"OpenAssistant is initialized \ud83d\udcaa\")\n",
        "if agent_name == \"OpenAI (API Key)\":\n",
        "    from transformers.tools import OpenAiAgent\n",
        "    pswd = openai_apikey\n",
        "    agent = OpenAiAgent(model=\"text-davinci-003\", api_key=pswd)\n",
        "    print(\"OpenAI is initialized \ud83d\udcaa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "caption = agent.run(\"Can you caption the `image`?\", image=img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"text\": caption,\n",
        "  \"model_id\": \"eleven_monolingual_v1\",\n",
        "  \"voice_settings\": {\n",
        "    \"stability\": 0.5,\n",
        "    \"similarity_boost\": 0.5\n",
        "  }\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data, headers=headers)\n",
        "with open('output.mp3', 'wb') as f:\n",
        "    for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
        "        if chunk:\n",
        "            f.write(chunk)\n",
        "\n",
        "audio_file = 'output.mp3'\n",
        "\n",
        "audio = Audio(filename=audio_file, autoplay =True)\n",
        "display(audio)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<a name=\"conclusion\"></a>\n",
        "- [Back to Contents](#contents)\n",
        "## Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "- Handle transactional and analytical queries with your vector data\n",
        "- no need to export data out of SingleStore to another vector db\n",
        "- Scan vectors fast with exact nearest neighbor. (DOT_PRODUCT, EUCLIDEAN_DISTANCE, and VECTOR_SUB are high-perf functions using single-instruction-multiple-data (SIMD) processor instructions)\n",
        "- Ability to stream data directly into SingleStore\n",
        "- Use SingleStore as Semantic Cache Layer leveraging the Plancache. No need for a cache layer.\n",
        "- Easily scale the workspace for your workload\n",
        "- handle reads and writes in parallel\n",
        "- Use of external functions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "## Reset Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "DROP DATABASE llm_webinar;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
