{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f8f86e0-d07b-40e6-8fb4-e343f06cc738",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(255, 167, 103, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/crystal-ball.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Build a GenAI app with SingleStoreDB and LlamaIndex</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Generative AI Application with LlamaIndex and SingleStore\n",
        "\n",
        "Welcome to this in-depth guide on constructing a Generative AI application utilizing LlamaIndex and SingleStoreDB. This guide will provide a step-by-step walkthrough, code explanations, and best practices.\n",
        "\n",
        "## Overview\n",
        "LlamaIndex is a library dedicated to ingesting, indexing, and querying contextual information for Retrieval Augmented Generation (RAG). In synergy with SingleStoreDB, a scalable and SQL-compliant relational database system, it lays the foundation for building powerful generative AI applications. This combination facilitates real-time data processing and retrieval, essential for answering user queries efficiently. LlamaIndex is also cross compatible with Langchain, another popular library used for composing LLM inputs and outputs. We'll use both with SingleStore to build an end-to-end GenAI app.\n",
        "\n",
        "## What You'll Learn\n",
        "- Setting up the environment with the required packages and credentials.\n",
        "- Ingesting and indexing data using LlamaIndex for efficient retrieval.\n",
        "- Storing and managing data in SingleStoreDB.\n",
        "- Building a retrieval-based generative AI system to respond to user queries.\n",
        "\n",
        "## Prerequisites\n",
        "- Basic knowledge of Python programming.\n",
        "- Understanding of SQL databases.\n",
        "- Familiarity with generative AI concepts would be beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first install the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index --quiet\n",
        "%pip install langchain --quiet\n",
        "%pip install llama-hub --quiet\n",
        "%pip install singlestoredb --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, let's set our OpenAI API Key. Note: the API keys used in this notebook are placeholders and invalid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll import the SingleStore vectorstore from Langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import SingleStoreDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing SingleStore, we can ingest the docs for LlamaIndex into a new table. This takes three steps:\n",
        "\n",
        "1. Load raw HTML data using WebBaseLoader\n",
        "2. Chunk the text.\n",
        "3. Embed or vectorize the chunked text, then ingest it into SingleStore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://gpt-index.readthedocs.io/en/latest/\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "os.environ[\"SINGLESTOREDB_URL\"] = \"admin:password@svc-56441794-b2ba-46ad-bc0b-c3d5810a45f4-dml.aws-oregon-3.svc.singlestore.com:3306/demo\"\n",
        "\n",
        "# vectorstore = SingleStoreDB.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "vectorstore = SingleStoreDB(embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll use Llama Index to retrieve and query from SingleStore using the SingleStoreReader, a lightweight embedding lookup tool for SingleStore databases ingested with content and vector data.\n",
        "\n",
        "Note that the full SingleStore vectorstore integration with Llama Index for ingesting and indexing is coming soon!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "SingleStoreReader = download_loader(\"SingleStoreReader\")\n",
        "\n",
        "reader = SingleStoreReader(\n",
        "    scheme=\"mysql\",\n",
        "    host=\"svc-56441794-b2ba-46ad-bc0b-c3d5810a45f4-dml.aws-oregon-3.svc.singlestore.com\",\n",
        "    port=\"3306\",\n",
        "    user=\"admin\",\n",
        "    password=\"password\",\n",
        "    dbname=\"demo\",\n",
        "    table_name=\"embeddings\",\n",
        "    content_field=\"content\",\n",
        "    vector_field=\"vector\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test it out. This function takes a natural language query as input, then does the following:\n",
        "\n",
        "1. Embed the query using the OpenAI Embedding model, `text-embedding-ada-002` by default.\n",
        "2. Ingest the documents into a Llama Index list index, a data structure that returns all documents into the context.\n",
        "3. Initialize the index as a Llama Index query engine, which uses the `gpt-3.5-turbo` OpenAI LLM by default to understand the query and provided context, then generate a response.\n",
        "4. Returns the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from llama_index import ListIndex\n",
        "\n",
        "def ask_llamaindex_docs(query):\n",
        "\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  search_embedding = embeddings.embed_query(query)\n",
        "  documents = reader.load_data(search_embedding=json.dumps(str(search_embedding)))\n",
        "\n",
        "  index = ListIndex(documents)\n",
        "\n",
        "  query_engine = index.as_query_engine()\n",
        "\n",
        "  response = query_engine.query(query)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ask_llamaindex_docs(\"What is Llama Index?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ask_llamaindex_docs(\"What are data indexes in Llama Index?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ask_llamaindex_docs(\"What are query engines in Llama Index?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tips and Tricks\n",
        "\n",
        "## Chat engines\n",
        "\n",
        "Chat with your data conversationally with Llama Index Chat Engines, which allow for follow-ups and further questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What is Llama Index?\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "search_embedding = embeddings.embed_query(query)\n",
        "documents = reader.load_data(search_embedding=json.dumps(str(search_embedding)))\n",
        "\n",
        "index = ListIndex(documents)\n",
        "\n",
        "chat_engine = index.as_chat_engine(chat_mode='context')\n",
        "\n",
        "chat_engine.chat_repl()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finetune Embeddings\n",
        "\n",
        "Improve your retrieval performance by 5-10% using a finetuned embedding model. Though a full implementation is outside the scope of this webinar, at a high level, you will:\n",
        "\n",
        "1. Split your data into train and validation datasets.\n",
        "2. Generate synthetic QA embedding pairs.\n",
        "3. Finetune your model using Llama Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your data goes here\n",
        "train_dataset = generate_qa_embedding_pairs(train_nodes)\n",
        "val_dataset = generate_qa_embedding_pairs(val_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.finetuning import SentenceTransformersFinetuneEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "finetune_engine = SentenceTransformersFinetuneEngine(\n",
        "    train_dataset,\n",
        "    model_id=\"BAAI/bge-small-en\",\n",
        "    model_output_path=\"test_model\",\n",
        "    val_dataset=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "finetune_engine.finetune()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_model = finetune_engine.get_finetuned_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Agents\n",
        "\n",
        "Data Agents are agents in LlamaIndex that can reason over your data and perform predefined tasks, with the ability to read and modify your data. They can:\n",
        "\n",
        "- Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n",
        "\n",
        "- Calling any external service API in a structured fashion, and processing the response + storing it for later.\n",
        "\n",
        "We'll create a simple agent with access to the `ask_llamaindex_docs` function we created earlier as a tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms import OpenAI\n",
        "from llama_index.agent import ReActAgent\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "llamaindex_docs_tool = QueryEngineTool(\n",
        "    query_engine=index.as_query_engine(),\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"llamaindex_docs\",\n",
        "        description=\"Provides access to the docs for Llama Index, a library for ingesting, indexing, and querying data for LLMs.\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = ReActAgent.from_tools([llamaindex_docs_tool], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.chat(\"What is Llama Index?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.chat(\"Tell me about it's capabiltiies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1dfe71-ce29-4b0c-86e9-e04418348534",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
