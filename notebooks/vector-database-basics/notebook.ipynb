{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1794de89-5ae3-44d3-bfa6-3d41c4f4deba",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(235, 249, 245, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/database.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">A Deep Dive Into Vector Databases</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Required Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai numpy pandas singlestoredb langchain==0.1.8 langchain-community==0.0.21 langchain-core==0.1.25 langchain-openai==0.0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Embedding Example\n",
        "\n",
        "In this example, we demonstrate a rule based system that generates vector embeddings based on a word. The embedding that we generate contains 5 main features:\n",
        "- Length of word\n",
        "- Number of vowels in the word (normalized to the length of the word)\n",
        "- Whether the word starts with a vowel (1) or not (0)\n",
        "- Whether the word ends with a vowel (1) or not (0)\n",
        "- Percentage of consonants in the word\n",
        "\n",
        "This is a simple implementation of a **rule** based system to demonstrate the essence of what vector embedding models do. However, they utlize neural networks that are trained on vast datasets to learn key features and self-corrects using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word_to_vector(word):\n",
        "    # Define some basic rules for our vector components\n",
        "    vector = [0] * 5  # Initialize a vector of 5 dimensions\n",
        "\n",
        "    # Rule 1: Length of the word (normalized to a max of 10 characters for simplicity)\n",
        "    vector[0] = len(word) / 10\n",
        "\n",
        "    # Rule 2: Number of vowels in the word (normalized to the length of the word)\n",
        "    vowels = 'aeiou'\n",
        "    vector[1] = sum(1 for char in word if char in vowels) / len(word)\n",
        "\n",
        "    # Rule 3: Whether the word starts with a vowel (1) or not (0)\n",
        "    vector[2] = 1 if word[0] in vowels else 0\n",
        "\n",
        "    # Rule 4: Whether the word ends with a vowel (1) or not (0)\n",
        "    vector[3] = 1 if word[-1] in vowels else 0\n",
        "\n",
        "    # Rule 5: Percentage of consonants in the word\n",
        "    vector[4] = sum(1 for char in word if char not in vowels and char.isalpha()) / len(word)\n",
        "\n",
        "    return vector\n",
        "\n",
        "# Example usage\n",
        "word = \"example\"\n",
        "vector = word_to_vector(word)\n",
        "print(f\"Word: {word}\\nVector: {vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Similarity Example\n",
        "\n",
        "In this example, we demonstrate a way to determine the similarity between two vectors. There are many techniques to find the similiarity between two vectors but one of the most popular ways is using **cosine similarity**. Consine similarity is the the dot product between the two vectors divided by the product of the vector's normals (magnitudes).\n",
        "\n",
        "This is just an example to show how vector databases search for similar vectors. The fundamental problem with a system like this is our rule-based embedding because it does not give us a semantic understanding of the word/sentences/paragraphs. Instead, it gives us a classification of a single word's structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vector_a, vector_b):\n",
        "    # Calculate the dot product of vectors\n",
        "    dot_product = np.dot(vector_a, vector_b)\n",
        "    # Calculate the norm (magnitude) of each vector\n",
        "    norm_a = np.linalg.norm(vector_a)\n",
        "    norm_b = np.linalg.norm(vector_b)\n",
        "    # Calculate cosine similarity\n",
        "    similarity = dot_product / (norm_a * norm_b)\n",
        "    return similarity\n",
        "\n",
        "# Example usage\n",
        "word1 = \"example\"\n",
        "word2 = \"sample\"\n",
        "vector1 = word_to_vector(word1)\n",
        "vector2 = word_to_vector(word2)\n",
        "\n",
        "# Calculate and print cosine similarity\n",
        "similarity_score = cosine_similarity(vector1, vector2)\n",
        "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Models\n",
        "\n",
        "In order to generate semantic understanding of language within vectors, embedding models are required. Embedding models are trained on vast corpus of language data. Training embedding models starts by initializing word embeddings with random vectors. Each word in the vocabulary is assigned a vector of real numbers. They use neural networks trained on large datasets to predict a word from its context (Continuous Bag of Words model) or to predict the context given a word (Skip-Gram model). During training, the model adjusts the word vectors to minimize some loss function, often related to the likelihood of observing a word given its context (or vice versa) through gradient descent.\n",
        "\n",
        "Examples of embedding models include Word2Vec, GloVe, BERT, OpenAI text-embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_KEY = \"INSERT OPENAI KEY\"\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "def openAIEmbeddings(input):\n",
        "  response = client.embeddings.create(\n",
        "      input=\"input\",\n",
        "      model=\"text-embedding-3-small\"\n",
        "  )\n",
        "  return response.data[0].embedding\n",
        "\n",
        "print(openAIEmbeddings(\"Golden Retreiver\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, this is a huge vector! Over 1000 dimensions just in this one vector. This is why it is important for us to have good dimensionality reduction techniques during the similarity searches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a vector database with SingleStoreDB\n",
        "\n",
        "In the following code we create a vector datbase with SingleStoreDB. We utilize Langchain to chunk and split the raw text into documents and use the OpenAI embeddings model to generate the vector embeddings. We then take the raw documents and embeddings and create a table with the columns \"docs\" and \"embeddings\".\n",
        "\n",
        "To test this out, we perform a similarity search based on a query and it returns the most similar document in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.singlestoredb import SingleStoreDB\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load and process documents\n",
        "loader = TextLoader(\"michael_jackson.txt\") # use your own document\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Generate embeddings and create a document search database\n",
        "embeddings = OpenAIEmbeddings(api_key=OPENAI_KEY)\n",
        "\n",
        "# Create Vector Database\n",
        "vector_database = SingleStoreDB.from_documents(docs, embeddings, table_name=\"mjackson\") # create your own table\n",
        "\n",
        "query = \"How old was Michael Jackson when he died?\"\n",
        "docs = vector_database.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval Augmented Generation System\n",
        "\n",
        "RAG combines large language models with a retrieval mechanism to search a database for relevant information before generating responses. It utilizes real-world data from retrieved documents to ground responses, enhancing factual accuracy and reducing hallucinations. Documents are vectorized using embeddings and stored in a vector database for efficient retrieval. SingleStoreDB serves as a great vector database. The user query is converted into a vector, and a vector search is performed in the database to find documents relevant to that specific query. The system returns the documents with the highest relevance scores, which are then fed to the chatbot for generating informed responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.singlestoredb import SingleStoreDB\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up API keys and database URL\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Load and process documents\n",
        "loader = TextLoader(\"michael_jackson.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Generate embeddings and create a document search database\n",
        "embeddings = OpenAIEmbeddings(OPENAI_KEY)\n",
        "docsearch = SingleStoreDB.from_documents(docs, embeddings, table_name=\"mjackson\")\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_query = input(\"\\nYou: \")\n",
        "\n",
        "    # Check for exit command\n",
        "    if user_query.lower() in ['quit', 'exit']:\n",
        "        print(\"Exiting chatbot.\")\n",
        "        break\n",
        "\n",
        "    # Perform similarity search\n",
        "    docs = docsearch.similarity_search(user_query)\n",
        "    if docs:\n",
        "        context = docs[0].page_content\n",
        "\n",
        "        # Generate response using OpenAI GPT-4\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Context: \" + context},\n",
        "                {\"role\": \"user\", \"content\": user_query}\n",
        "            ],\n",
        "            stream=True,\n",
        "            max_tokens=500,\n",
        "        )\n",
        "\n",
        "        # Output the response\n",
        "        print(\"AI: \", end=\"\")\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content is not None:\n",
        "                print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "    else:\n",
        "        print(\"AI: Sorry, I couldn't find relevant information.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03050224-26e0-4015-a05c-a5025fd233d0",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
