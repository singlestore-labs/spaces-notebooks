{
  "cells": [
    {
      "id": "255b1ca3",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(124, 195, 235, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/arrows-spin.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Resume Evaluator</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "id": "e8993583",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Note</b></p>\n",
        "        <p>This notebook can be run on a Free Starter Workspace. To create a Free Starter Workspace navigate to <tt>Start</tt> using the left nav. You can also use your existing Standard or Premium workspace with this Notebook.</p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will showcase a practical use-case of evaluating resumes leveraging the combined powers of SingleStoreDB and OpenAI. Moving beyond traditional resume-matching techniques, we introduce a method that dives deeper into the nuances of resume content and the specific needs of a job description.\n",
        "\n",
        "With SingleStoreDB's innate ability to swiftly manage and query data, we begin by ingesting a resume PDF, extracting its textual content, and storing it in a dedicated database table. But we don\u2019t stop there! The resume's text is then processed by OpenAI's LLM, summarizing its key points. This summarized version, along with its vector representation, is saved back to our SingleStoreDB table.\n",
        "\n",
        "When it's time to match a resume to a job description, we translate the latter into its vector form. Using a dot_product operation, we search against the table housing the resume summaries' embeddings. This provides us with resumes most aligned with the job description. To add another layer of precision, the matched resumes are further evaluated alongside the job description using OpenAI's LLM, offering a comprehensive assessment.\n",
        "\n",
        "Join us on this journey to redefine resume evaluations using SQL, SingleStoreDB, and OpenAI for better, more insightful matches!"
      ],
      "id": "8e1fef21"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create a workspace in your workspace group\n",
        "\n",
        "S-00 is sufficient.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> If you have a Free Starter Workspace deployed already, select the database from drop-down menu at the top of this notebook. It updates the <tt>connection_url</tt> to connect to that database.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "## 2. Create a database named resume_evaluator"
      ],
      "id": "dd26de8e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_tier_check = %sql show variables like 'is_shared_tier'\n",
        "if not shared_tier_check or shared_tier_check[0][1] == 'OFF':\n",
        "    %sql DROP DATABASE IF EXISTS resume_evaluator;\n",
        "    %sql CREATE DATABASE resume_evaluator;"
      ],
      "id": "c7deae6d"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p>Make sure to select the <tt>resume_evaluator</tt> database from the drop-down menu at the top of this notebook.\n",
        "        It updates the <tt>connection_url</tt> which is used by the <tt>%%sql</tt> magic command and SQLAlchemy to make connections to the selected database.</p>\n",
        "    </div>\n",
        "</div>"
      ],
      "id": "f023a713"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS resumes_profile_data /* Creating table for sample data. */(\n",
        "    names text,\n",
        "    email text,\n",
        "    phone_no text,\n",
        "    years_of_experience text,\n",
        "    skills text,\n",
        "    profile_name text,\n",
        "    resume_summary text,\n",
        "    resume_embeddings blob\n",
        ");"
      ],
      "id": "b8bf5006"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install and import required libraries\n",
        "\n",
        "In this section, we will set up the necessary environment by installing some crucial libraries. For our task of extracting text from resume PDFs, we'll be using pdfminer.six. To interact with the OpenAI's LLM and manage our data efficiently, openai will be instrumental.\n",
        "\n",
        "The install process may take a couple minutes."
      ],
      "id": "b066170a"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pdfminer.six openai==1.3.3"
      ],
      "id": "0a911696"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from openai import OpenAI\n",
        "from pdfminer.high_level import extract_text\n",
        "from singlestoredb import create_engine\n",
        "from sqlalchemy import text"
      ],
      "id": "a08db39b"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n",
        "client = openai.OpenAI()"
      ],
      "id": "03b1ae45"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create a function called `get_embedding()`\n",
        "In our workflow, we need a consistent way to transform textual content into vector embeddings. To achieve this, we introduce the get_embedding() function.\n",
        "\n",
        "This function takes in a piece of text and, by default, uses the \"text-embedding-ada-002\" model to produce embeddings. We ensure that any newline characters in the text are replaced with spaces to maintain the integrity of the input. The function then leverages OpenAI's API to generate and retrieve the embedding for the given text."
      ],
      "id": "6cfdcf1f"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = openai.embeddings.create(input=[text], model=model)\n",
        "    return response.data[0].embedding"
      ],
      "id": "e1d61bed"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create a function called `print_pdf_text()`\n",
        "\n",
        "This function is designed to extract and clean the text from a provided PDF, either from a web URL or a local file path.\n",
        "\n",
        "#### Input:\n",
        "- `url`: Web URL of the PDF (optional).\n",
        "- `file_path`: Local path of the PDF (optional).\n",
        "\n",
        "#### Functionality:\n",
        "- **Source Determination**:\n",
        "  - Fetches PDF from `url` or uses the local `file_path`.\n",
        "  - Raises an error if neither is provided.\n",
        "- **Text Extraction**: Extracts text from the PDF using `pdfminer`.\n",
        "- **Text Cleaning**:\n",
        "  - Removes special characters, retaining only \"@\", \"+\", \".\", and \"/\".\n",
        "  - Improves formatting by handling newline characters.\n",
        "- **Cleanup**: If a temporary file was created from a URL, it gets deleted post-processing.\n",
        "\n",
        "#### Output:\n",
        "Returns the cleaned and formatted text from the PDF."
      ],
      "id": "65ba6010"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_pdf_text(url=None, file_path=None):\n",
        "    # Determine the source of the PDF (URL or local file)\n",
        "    if url:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "        temp_file_path = \"temp_pdf_file.pdf\"\n",
        "        with open(temp_file_path, 'wb') as temp_file:\n",
        "            temp_file.write(response.content)  # Save the PDF to a temporary file\n",
        "        pdf_source = temp_file_path\n",
        "    elif file_path:\n",
        "        pdf_source = file_path  # Set the source to the provided local file path\n",
        "    else:\n",
        "        raise ValueError(\"Either url or file_path must be provided.\")\n",
        "\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(pdf_source)\n",
        "\n",
        "    # Remove special characters except \"@\", \"+\", \".\", and \"/\"\n",
        "    cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s@+./:,]\", \"\", text)\n",
        "\n",
        "    # Format the text for better readability\n",
        "    cleaned_text = cleaned_text.replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \")\n",
        "    # If a temporary file was used, delete it\n",
        "    if url and os.path.exists(temp_file_path):\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "    return cleaned_text"
      ],
      "id": "c64e8265"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create a function called `pinfo_extractor()`\n",
        "\n",
        "This function is tailored to extract specific details from a candidate's resume text.\n",
        "\n",
        "### Input:\n",
        "- `resume_text`: The text extracted from a candidate's resume.\n",
        "\n",
        "### Functionality:\n",
        "- **Prompt Creation**:\n",
        "  - A context is formed using the provided resume text.\n",
        "  - A detailed question prompt is generated to guide the extraction of desired details from the resume.\n",
        "- **OpenAI API Interaction**:\n",
        "  - Uses the `gpt-3.5-turbo` model to process the prompt and generate a detailed extraction.\n",
        "  - Extracts relevant sections like Name, Email, Phone Number, and more from the generated response.\n",
        "- **Data Structuring**:\n",
        "  - The extracted details are organized into a dictionary.\n",
        "\n",
        "### Output:\n",
        "Returns a dictionary with keys such as 'name', 'email', 'phone_no', and more, containing extracted information from the resume."
      ],
      "id": "3f50e333"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pinfo_extractor(resume_text):\n",
        "    context = f\"Resume text: {resume_text}\"\n",
        "    question = \"\"\" From above candidate's resume text, extract the only following details:\n",
        "                Name: (Find the candidate's full name. If not available, specify \"not available.\")\n",
        "                Email: (Locate the candidate's email address. If not available, specify \"not available.\")\n",
        "                Phone Number: (Identify the candidate's phone number. If not found, specify \"not available.\")\n",
        "                Years of Experience: (If not explicitly mentioned, calculate the years of experience by analyzing the time durations at each company or position listed. Sum up the total durations to estimate the years of experience. If not determinable, write \"not available.\")\n",
        "                Skills Set: Extract the skills which are purely technical and represent them as: [skill1, skill2,... <other skills from resume>]. If no skills are provided, state \"not available.\"\n",
        "                Profile: (Identify the candidate's job profile or designation. If not mentioned, specify \"not available.\")\n",
        "                Summary: provide a brief summary of the candidate's profile without using more than one newline to segregate sections.\n",
        "                \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        Based on the below given candidate information, only answer asked question:\n",
        "        {context}\n",
        "        Question: {question}\n",
        "    \"\"\"\n",
        "    # print(prompt)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful HR recruiter.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=700,\n",
        "        temperature=0.5,\n",
        "        n=1  # assuming you want one generation per document\n",
        "    )\n",
        "    # Extract the generated response\n",
        "    response_text = response.choices[0].message.content # response['choices'][0]['message']['content']\n",
        "    print(response_text)\n",
        "    # Split the response_text into lines\n",
        "    lines = response_text.strip().split('\\n')\n",
        "\n",
        "    # Now, split each line on the colon to separate the labels from the values\n",
        "    # Extract the values\n",
        "    name = lines[0].split(': ')[1]\n",
        "    email = lines[1].split(': ')[1]\n",
        "    phone_no = lines[2].split(': ')[1]\n",
        "    years_of_expiernce = lines[3].split(': ')[1]\n",
        "    skills = lines[4].split(': ')[1]\n",
        "    profile = lines[5].split(': ')[1]\n",
        "    summary = lines[6].split(': ')[1]\n",
        "    data_dict = {\n",
        "        'name': name,\n",
        "        'email': email,\n",
        "        'phone_no': phone_no,\n",
        "        'years_of_expiernce': years_of_expiernce,\n",
        "        'skills': skills,\n",
        "        'profile': profile,\n",
        "        'summary': summary\n",
        "    }\n",
        "    print(data_dict, \"\\n\")\n",
        "    return data_dict;"
      ],
      "id": "fdf815ca"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create a function called `add_data_to_db()`\n",
        "\n",
        "This function is designed to add extracted resume details into a database.\n",
        "\n",
        "### Input:\n",
        "- `input_dict`: Dictionary containing details like 'name', 'email', 'phone_no', and more extracted from a resume.\n",
        "\n",
        "### Functionality:\n",
        "- **Database Connection**:\n",
        "  - Establishes a connection to the database using SQLAlchemy's `create_engine` with the given connection URL.\n",
        "- **Embedding Creation**:\n",
        "  - Calls the `get_embedding()` function to generate an embedding for the resume summary.\n",
        "- **SQL Query Formation**:\n",
        "  - Crafts an SQL query to insert the provided data (from the input dictionary) into the `resumes_profile_data` table in the database.\n",
        "- **Data Insertion**:\n",
        "  - Opens a connection, executes the SQL query, commits the changes, and then closes the connection.\n",
        "\n",
        "### Output:\n",
        "Prints a confirmation message upon successful data insertion."
      ],
      "id": "13436642"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_data_to_db(input_dict):\n",
        "    # Create the SQLAlchemy engine\n",
        "    # engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database_name}')\n",
        "\n",
        "    engine = create_engine(connection_url)\n",
        "    # Get the embedding for the summary text\n",
        "    summary = input_dict['summary']\n",
        "    embedding = get_embedding(summary)\n",
        "    # Create the SQL query for inserting the data\n",
        "    query_sql = f\"\"\"\n",
        "        INSERT INTO resumes_profile_data (names, email, phone_no, years_of_experience, skills, profile_name, resume_summary, resume_embeddings)\n",
        "        VALUES (\"{input_dict['name']}\", \"{input_dict['email']}\", \"{input_dict['phone_no']}\", \"{input_dict['years_of_expiernce']}\",\n",
        "        \"{input_dict['skills']}\", \"{input_dict['profile']}\", \"{summary}\", JSON_ARRAY_PACK('{embedding}'));\n",
        "    \"\"\"\n",
        "    with engine.connect() as connection:\n",
        "        connection.execute(text(query_sql))\n",
        "        connection.commit()\n",
        "    print(\"\\nData Written to resumes_profile_data_2 table\")"
      ],
      "id": "ecf9721c"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create a function called `search_resumes()`\n",
        "\n",
        "This function facilitates the search for resumes that are most similar to a given query, leveraging embeddings and database operations.\n",
        "\n",
        "### Input:\n",
        "- `query`: A string that represents the job description or any other search criteria.\n",
        "\n",
        "### Functionality:\n",
        "- **Embedding Creation**:\n",
        "  - Converts the given `query` into its corresponding embedding using the `get_embedding()` function.\n",
        "- **SQL Query Formation**:\n",
        "  - Creates an SQL query to search for the top 5 resumes in the `resumes_profile_data` table that have the highest similarity (dot product) to the query embedding.\n",
        "- **Database Operations**:\n",
        "  - Opens a connection to the database, runs the SQL query to fetch the results, and then closes the connection.\n",
        "\n",
        "### Output:\n",
        "Returns a list of the top 5 most relevant resumes based on the given query."
      ],
      "id": "7a0c032a"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_resumes(query):\n",
        "    query_embed = get_embedding(query)\n",
        "    query_sql = f\"\"\"\n",
        "            SELECT names, resume_summary, dot_product(\n",
        "                    JSON_ARRAY_PACK('{query_embed}'),\n",
        "                    resume_embeddings\n",
        "                ) AS similarity\n",
        "                FROM resumes_profile_data\n",
        "                ORDER BY similarity DESC\n",
        "                LIMIT 5;\n",
        "    \"\"\"\n",
        "    # print(query_sql,\"\\n\")\n",
        "    # engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database_name}')\n",
        "    engine = create_engine(connection_url)\n",
        "    connection = engine.connect()\n",
        "    result = connection.execute(text(query_sql)).fetchall()\n",
        "    connection.close()\n",
        "    engine.dispose()\n",
        "    return result"
      ],
      "id": "b071d666"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create a function called `evaluate_candidates()`\n",
        "\n",
        "This function is geared towards evaluating the compatibility of candidates' resumes in relation to a specific job description.\n",
        "\n",
        "### Input:\n",
        "- `query`: A string that represents the job description against which candidates' resumes will be assessed.\n",
        "\n",
        "### Functionality:\n",
        "- **Resume Retrieval**:\n",
        "  - Utilizes the `search_resumes()` function to get the top matching resumes based on the job description.\n",
        "- **OpenAI API Interaction**:\n",
        "  - For each retrieved resume, a prompt is crafted, asking to evaluate how well the candidate fits the job description.\n",
        "  - Interacts with the `gpt-3.5-turbo` model to process this prompt and receive an efficient, concise response.\n",
        "- **Data Aggregation**:\n",
        "  - Collects the model's evaluation responses for each candidate in a list.\n",
        "\n",
        "### Output:\n",
        "Returns a list of tuples, where each tuple contains:\n",
        "- Candidate's name.\n",
        "- Evaluation response from the model, describing the compatibility of the candidate with the given job description."
      ],
      "id": "590c1d7e"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_candidates(query):\n",
        "    result = search_resumes(query)\n",
        "    responses = []  # List to store responses for each candidate\n",
        "    for resume_str in result:\n",
        "        name = resume_str[0]\n",
        "        context = f\"Resume text: {resume_str[1]}\"\n",
        "        question = f\"What percentage of the job requirements does the candidate meet for the following job description? answer in 3 lines only and be effcient while answering: {query}.\"\n",
        "        prompt = f\"\"\"\n",
        "            Read below candidate information about the candidate:\n",
        "            {context}\n",
        "            Question: {question}\n",
        "        \"\"\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a expert HR analyst and recuriter.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            temperature=0.2,\n",
        "            n=1  # assuming you want one generation per document\n",
        "        )\n",
        "        # Extract the generated response\n",
        "        response_text = response.choices[0].message.content # response['choices'][0]['message']['content']\n",
        "        responses.append((name, response_text))  # Append the name and response_text to the responses list\n",
        "    return responses"
      ],
      "id": "0ffb2c38"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://github.com/vishwajeetdabholkar/vishwajeet-resume/raw/main/Vishwajeet_Dabholkar_Resume_June_2023.pdf\"\n",
        "]\n",
        "\n",
        "for url in urls:\n",
        "    resume_text = print_pdf_text(url=url).replace('\\n',' ')\n",
        "    print(\"Resume Text extracted\\n\")\n",
        "    ip_data_dict = pinfo_extractor(resume_text)\n",
        "    print(\"Information extracted\\n\")\n",
        "    add_data_to_db(ip_data_dict)\n",
        "    print(\"\\n\")"
      ],
      "id": "4a5d926d"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT * FROM resumes_profile_data;"
      ],
      "id": "7fd89294"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_description = input(\"Enter Job description : \\n\")\n",
        "evaluate_candidates(job_description)"
      ],
      "id": "dc6e863a"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup"
      ],
      "id": "8ff0fcbc"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> If you created a new database in your Standard or Premium Workspace, you can drop the database by running the cell below. Note: this will not drop your database for Free Starter Workspaces. To drop a Free Starter Workspace, terminate the Workspace using the UI. </p>\n",
        "    </div>\n",
        "</div>"
      ],
      "id": "ddc98af3"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_tier_check = %sql show variables like 'is_shared_tier'\n",
        "if not shared_tier_check or shared_tier_check[0][1] == 'OFF':\n",
        "    %sql DROP DATABASE IF EXISTS resume_evaluator;"
      ],
      "id": "7e473c11"
    },
    {
      "id": "522b95f0",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
