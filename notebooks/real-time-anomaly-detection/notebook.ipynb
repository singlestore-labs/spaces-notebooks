{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "951e6991-8cdd-453d-bad4-dd1472d29ff7",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(124, 195, 235, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/chart-scatter.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Real-Time Anomaly Detection</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3238cf7d-5a34-4463-8bf7-f64f89fc3055",
      "metadata": {},
      "source": [
        "In this notebook, we embark on a cutting-edge exploration of real-time anomaly detection in IoT sensor data, harnessing the robust capabilities of SingleStoreDB and advanced analytical techniques. Our journey begins with the efficient ingestion of sensor data into SingleStoreDB, setting the stage for dynamic and insightful analysis. The heart of this notebook lies in its innovative approach to handling and interpreting sensor data. We utilize the power of vector embeddings, generated through the UMAP library, to transform high-dimensional sensor readings into a format ripe for anomaly detection. These embeddings, capturing the essence of weather parameters like wind, rain, and temperature, are then seamlessly integrated into SingleStoreDB.\n",
        "\n",
        "Our focus intensifies as we apply SingleStoreDB's dot_product function to these embeddings, unearthing anomalies in real-time. This not only provides a swift identification of irregularities but also paints a vivid picture of sensor data behavior over time. We don\u2019t just stop at detection; the notebook further enriches the data analysis with a visually engaging, real-time dashboard. This dashboard, crafted using Plotly and Rich libraries, offers an interactive and constantly updated view of the anomalies, allowing users to monitor and respond to sensor data trends as they happen. Join us in this exciting venture as we blend SQL, SingleStoreDB, and Python to unlock new possibilities in real-time anomaly detection. Whether you're a data scientist, an IoT enthusiast, or simply intrigued by the power of real-time analytics, this notebook is your gateway to understanding and leveraging the full potential of IoT sensor data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3df232a-aee8-4d16-ac9b-035336e2e1ad",
      "metadata": {},
      "source": [
        "<center>\n",
        "    <img src=\"images/architecture.png\" alt=\"Architecture Diagram\" width=\"800\" />\n",
        "</center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "73457163-c407-4886-bb54-b869d22a0bea",
      "metadata": {},
      "source": [
        "## Database Setup\n",
        "\n",
        "### Overview\n",
        "This section focuses on the initial setup of the database `iot_sensor_db` in SingleStore, specifically designed for handling IoT sensor data. It includes creating the necessary tables to store both historical and real-time sensor data, along with vector embeddings for anomaly detection.\n",
        "\n",
        "### SQL Script Description\n",
        "The provided SQL script performs the following operations:\n",
        "\n",
        "1. **Database Initialization**\n",
        "   - `DROP DATABASE IF EXISTS iot_sensor_db;`\n",
        "     Ensures a clean slate by dropping the `iot_sensor_db` database if it already exists.\n",
        "   - `CREATE DATABASE iot_sensor_db;`\n",
        "     Creates a new database named `iot_sensor_db`.\n",
        "   - `USE iot_sensor_db;`\n",
        "     Sets `iot_sensor_db` as the current database for subsequent operations.\n",
        "\n",
        "2. **Table Creation**\n",
        "   - **`CREATE TABLE sensor_data_with_vectors`**\n",
        "     This table is designed to store processed sensor data along with vector embeddings for anomaly detection.\n",
        "     - **Columns**:\n",
        "       - `date`: Timestamp of the sensor data.\n",
        "       - `city`, `longitude`, `latitude`: Location information of the sensor.\n",
        "       - `vent`, `pluie`, `temp`: Sensor readings for wind (vent), rain (pluie), and temperature (temp).\n",
        "       - `anomaly`: Flag indicating whether the data point is an anomaly.\n",
        "       - `embeddings`: Text column for storing vector embeddings.\n",
        "\n",
        "   - **`CREATE TABLE sensor_data_stage`**\n",
        "     Serves as a staging area for raw sensor data before processing.\n",
        "     - **Columns**: Similar to `sensor_data_with_vectors`, but used for staging raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21f4b459-f1cf-4419-94be-7a32cbe5062d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "DROP DATABASE IF EXISTS iot_sensor_db;\n",
        "\n",
        "CREATE DATABASE iot_sensor_db;\n",
        "\n",
        "USE iot_sensor_db;\n",
        "\n",
        "CREATE TABLE sensor_data_with_vectors (\n",
        "  date DATETIME,\n",
        "    city VARCHAR(50),\n",
        "    longitude VARCHAR(50),\n",
        "    latitude VARCHAR(50),\n",
        "  vent FLOAT(8,2),\n",
        "  pluie FLOAT(8,2),\n",
        "  temp FLOAT(8,2),\n",
        "  anomaly VARCHAR(10),\n",
        "  embeddings text\n",
        ");\n",
        "\n",
        "CREATE TABLE sensor_data_stage (\n",
        "    city VARCHAR(50),\n",
        "    longitude VARCHAR(50),\n",
        "    latitude VARCHAR(50),\n",
        "    vent FLOAT(8,2),\n",
        "    pluie FLOAT(8,2),\n",
        "    temp FLOAT(8,2),\n",
        "    embeddings text,\n",
        "    date DATETIME\n",
        ");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d9b24a3c-0c7b-4f3d-afbd-38fdae6c8d74",
      "metadata": {},
      "source": [
        "## Setting Up and Initiating the Sensor Data Pipeline\n",
        "\n",
        "### Overview\n",
        "This section details the setup and initiation of two pipelines in SingleStore: `sensor_data_pipeline` for historical data load and `sensor_realtime_data_pipeline` for real-time data analysis. Both pipelines stream and ingest IoT sensor data from S3 buckets into respective tables in SingleStore.\n",
        "\n",
        "### SQL Script Description\n",
        "\n",
        "#### Historical Data Load Pipeline\n",
        "1. **Pipeline Creation**\n",
        "   - **`CREATE OR REPLACE PIPELINE sensor_data_pipeline AS`**\n",
        "     Creates or replaces a pipeline named `sensor_data_pipeline`.\n",
        "   - **Configuration**:\n",
        "     - Source: S3 bucket path `s3://real-time-anomaly-detection-demo/demothon/with_cities_embeddings.csv`.\n",
        "     - Target: Table `sensor_data_with_vectors`.\n",
        "     - Data Format: CSV with specific delimiters and header line ignored.\n",
        "\n",
        "2. **Pipeline Activation**\n",
        "   - **`START PIPELINE sensor_data_pipeline FOREGROUND;`**\n",
        "     Initiates the pipeline for data ingestion.\n",
        "\n",
        "3. **Data Verification**\n",
        "   - **`SELECT * FROM sensor_data_with_vectors limit 2;`**\n",
        "     Fetches the first two rows from `sensor_data_with_vectors` to verify data ingestion.\n",
        "\n",
        "#### Real-Time Data Analysis Pipeline\n",
        "1. **Pipeline Creation**\n",
        "   - **`CREATE OR REPLACE PIPELINE sensor_realtime_data_pipeline AS`**\n",
        "     Establishes a new pipeline named `sensor_realtime_data_pipeline`.\n",
        "   - **Configuration**:\n",
        "     - Source: S3 bucket path `s3://real-time-anomaly-detection-demo/demothon/demo_day_data2.csv`.\n",
        "     - Target: Table `sensor_data_stage`.\n",
        "     - Data Format: CSV with specific delimiters and header line ignored.\n",
        "     - Additional Setting: `SET date = NOW();` assigns the current timestamp to the `date` column.\n",
        "\n",
        "2. **Pipeline Activation**\n",
        "   - **`START PIPELINE sensor_realtime_data_pipeline FOREGROUND;`**\n",
        "     Activates the pipeline for real-time data ingestion.\n",
        "\n",
        "3. **Data Verification**\n",
        "   - **`SELECT * FROM sensor_data_stage limit 1;`**\n",
        "     Retrieves the first row from `sensor_data_stage` to confirm data ingestion.\n",
        "\n",
        "### Usage\n",
        "The establishment of these pipelines is essential for the real-time and historical analysis of IoT sensor data. `sensor_data_pipeline` facilitates the ingestion of historical data for retrospective analysis, while `sensor_realtime_data_pipeline` caters to ongoing, real-time data analysis needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "261175c2-9a3f-4b42-b542-e9216f6df555",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "CREATE OR REPLACE PIPELINE sensor_data_pipeline AS\n",
        "LOAD DATA S3 's3://s2db-demos-pub-bucket/real-time-anomaly-detection-demo/demothon/with_cities_embeddings.csv'\n",
        "INTO TABLE `sensor_data_with_vectors`\n",
        "FIELDS TERMINATED BY ','\n",
        "ENCLOSED BY '\"'\n",
        "LINES TERMINATED BY '\\n'\n",
        "IGNORE 1 LINES;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2af087a8-2925-4808-bbe6-4fc224f3b083",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "START PIPELINE sensor_data_pipeline FOREGROUND;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "14befa3d-637a-40ea-88bc-f27e8197e82d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT * FROM sensor_data_with_vectors limit 2;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "50b4cc6a-e047-41c7-8dd9-95b6c5ef9b2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE PIPELINE sensor_realtime_data_pipeline AS\n",
        "LOAD DATA S3 's3://s2db-demos-pub-bucket/real-time-anomaly-detection-demo/demothon/demo_day_data2.csv'\n",
        "INTO TABLE `sensor_data_stage`\n",
        "(city, longitude, latitude, vent, pluie, temp, embeddings)\n",
        "FIELDS TERMINATED BY ','\n",
        "ENCLOSED BY '\"'\n",
        "LINES TERMINATED BY '\\r\\n'\n",
        "IGNORE 1 LINES\n",
        "SET date = NOW();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "efa7eaf0-8238-4c9e-87df-861979fae434",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "START PIPELINE sensor_realtime_data_pipeline FOREGROUND;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d3b777a0-7583-41d3-aaa2-f52a2cc45d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT * FROM sensor_data_stage limit 1;"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "495f261f-7143-492e-a08f-9daa06845db9",
      "metadata": {},
      "source": [
        "## Data Preparation for Analysis\n",
        "\n",
        "### Overview\n",
        "This section covers the necessary steps to prepare IoT sensor data for analysis. It involves installing a required library, data retrieval from the `sensor_data_stage` table, and preprocessing to ensure data quality.\n",
        "\n",
        "### Python Script Description\n",
        "\n",
        "1. **Library Installation**\n",
        "   - **`!pip install umap-learn --quiet`**\n",
        "     Installs the `umap-learn` library quietly without verbose output. UMAP (Uniform Manifold Approximation and Projection) is used for dimensionality reduction.\n",
        "\n",
        "2. **Import Statements**\n",
        "   - **Note**: It's advised to restart the Python Kernel before importing `umap` to ensure the library is properly loaded.\n",
        "   - Imports various libraries including `umap`, `normalize` from `sklearn.preprocessing`, `sqlalchemy`, `create_engine`, `json`, and `pandas`.\n",
        "\n",
        "3. **Database Connection**\n",
        "   - **`engine = create_engine(connection_url)`**\n",
        "     Establishes a connection to the database using `connection_url`.\n",
        "\n",
        "4. **Data Retrieval and Preprocessing**\n",
        "   - **`df = pd.read_sql('select * from sensor_data_stage', engine)`**\n",
        "     Retrieves data from the `sensor_data_stage` table into a pandas DataFrame.\n",
        "   - **`df = df.bfill(axis=0)`**\n",
        "     Fills null values in the DataFrame by propagating non-null values backward.\n",
        "   - **`df = df.dropna()`**\n",
        "     Drops any remaining rows with null values to ensure the dataset is clean for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "672ea6de-3785-485c-9779-bd90f4538625",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install umap-learn --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "08f0b338-2c66-42d2-bd08-8613c3b6ecfc",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Note</b></p>\n",
        "        <p>Restart Kernel if importing umap gives error</p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ee9612f6-0598-4730-b01c-939f184498f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import umap\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "869ebb07-2294-4279-8470-f695b8d94b04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filling null values usingbfill()\n",
        "\n",
        "engine = create_engine(connection_url)\n",
        "\n",
        "df = pd.read_sql('select * from sensor_data_stage', engine)\n",
        "\n",
        "df = df.bfill(axis=0)\n",
        "\n",
        "df = df.dropna()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ee1a3971-4006-4ef7-885f-d5bd8d67cbe5",
      "metadata": {},
      "source": [
        "## Generating Vector Embeddings using UMAP Library\n",
        "\n",
        "### Overview\n",
        "This section focuses on creating vector embeddings from sensor data using the UMAP library. The embeddings are generated to reduce the dimensionality of the data while preserving its structure, aiding in efficient analysis.\n",
        "\n",
        "### Python Script Description\n",
        "\n",
        "1. **Data Selection for Embedding Generation**\n",
        "   - **`new_df1 = df.iloc[50:100]`**\n",
        "     Creates a subset of the DataFrame `df`, selecting rows 50 to 100. This subset is used for generating vector embeddings.\n",
        "\n",
        "2. **Feature Selection**\n",
        "   - **`features = new_df1[['vent', 'pluie', 'temp']]`**\n",
        "     Selects the columns `vent`, `pluie`, and `temp` from `new_df1` as features for the embedding process. These represent sensor readings for wind, rain, and temperature.\n",
        "\n",
        "3. **UMAP Reducer Initialization and Transformation**\n",
        "   - **`reducer = umap.UMAP(n_components=15)`**\n",
        "     Initializes a UMAP reducer to reduce the feature space to 15 components.\n",
        "   - **`embeddings = reducer.fit_transform(features)`**\n",
        "     Applies the UMAP transformation to the selected features, generating low-dimensional embeddings from the high-dimensional sensor data.\n",
        "\n",
        "4. **Normalization and Embedding Storage**\n",
        "   - **`normalized_embeddings = normalize(embeddings, norm='l2')`**\n",
        "     Normalizes the generated embeddings using L2 norm, ensuring uniform scale.\n",
        "   - **`new_df1['embeddings'] = list(normalized_embeddings)`**\n",
        "     Appends the normalized embeddings as a new column to `new_df1`.\n",
        "\n",
        "5. **Displaying Results**\n",
        "   - **`new_df1.head()`**\n",
        "     Displays the first few rows of `new_df1` to verify the embedding generation and integration process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "70bc031c-cc4a-4163-ab3c-d03b9591455a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# code to generate embeddings for real time data\n",
        "new_df1 = df.iloc[50:100]\n",
        "features = new_df1[['vent', 'pluie', 'temp']]\n",
        "\n",
        "reducer = umap.UMAP(n_components=15)\n",
        "embeddings = reducer.fit_transform(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ea92368c-16c6-41c0-ba42-0de5603f0d47",
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_embeddings = normalize(embeddings, norm='l2')\n",
        "new_df1['embeddings'] = list(normalized_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5e9d01b2-1142-4a91-b384-c2e3e82ca4be",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df1.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "83031520-8beb-42e5-8189-78fbef2ab67b",
      "metadata": {},
      "source": [
        "## Anomaly Detection and Data Integration\n",
        "\n",
        "### Anomaly Detection Using SingleStore dot_product Function\n",
        "- **Anomaly Detection Loop**:\n",
        "  - Iterates over each row in `new_df`.\n",
        "  - Extracts embeddings and converts them into JSON format.\n",
        "  - Constructs an SQL query using SingleStore's `dot_product` function to measure similarity between the current row's embeddings and those in the `sensor_data_with_vectors` table.\n",
        "  - The query fetches the `anomaly` status based on the highest similarity scores.\n",
        "  - SQL query execution: `result = pd.read_sql_query(query, con=engine)`.\n",
        "  - Anomalies are appended to `new_df` or set to a default value if no similar records are found.\n",
        "\n",
        "### Data Appending to Historical Table\n",
        "- **Data Type Casting**:\n",
        "  - Ensures appropriate data types for columns in `new_df` (e.g., converting `date` to datetime, `city`, `longitude`, `latitude` to strings, etc.).\n",
        "- **Appending to SQL Table**:\n",
        "  - `new_df.to_sql('sensor_data_with_vectors', con=engine, if_exists='append', index=False)` appends the processed data in `new_df` to the `sensor_data_with_vectors` table in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0a4d4069-5699-45fa-b0de-393e73271053",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = df.iloc[50:70].copy()\n",
        "# iterate over each row in the new DataFrame\n",
        "for index, row in new_df.iterrows():\n",
        "    # get the embeddings from the current row\n",
        "    embeddings = row['embeddings']\n",
        "\n",
        "    # convert numpy array to list and then to a JSON string\n",
        "    embeddings_json = json.loads(embeddings)\n",
        "\n",
        "    # create the query string\n",
        "    query = f\"\"\"\n",
        "           SELECT anomaly, COUNT(anomaly) as count\n",
        "            FROM (\n",
        "                SELECT anomaly, dot_product(\n",
        "                    JSON_ARRAY_PACK('{embeddings_json}'),\n",
        "                    JSON_ARRAY_PACK(sensor_data_with_vectors.embeddings)\n",
        "                ) AS similarity\n",
        "                FROM sensor_data_with_vectors\n",
        "                ORDER BY similarity DESC\n",
        "                LIMIT 20\n",
        "            ) AS subquery\n",
        "            GROUP BY anomaly\n",
        "            ORDER BY count DESC;\n",
        "    \"\"\"\n",
        "\n",
        "    # execute the query\n",
        "    result = pd.read_sql_query(query, con=engine)\n",
        "\n",
        "    # check if the result is empty\n",
        "    if not result.empty:\n",
        "        # append the result to the current row in the new DataFrame\n",
        "        new_df.loc[index, 'anomaly'] = result['anomaly'].values[0]\n",
        "    else:\n",
        "        # set anomaly to None or some default value\n",
        "        new_df.loc[index, 'anomaly'] = 'none'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7f9ff388-6b80-4f10-878b-48fd7e65ea66",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3f279837-caf1-47aa-8334-efa71a5c5e9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# appending the new dataframe to main the table : sensor_data_with_vectors\n",
        "new_df['date'] = pd.to_datetime(new_df['date'])\n",
        "new_df['city'] = new_df['city'].astype(str)\n",
        "new_df['longitude'] = new_df['longitude'].astype(str)\n",
        "new_df['latitude'] = new_df['latitude'].astype(str)\n",
        "new_df['vent'] = new_df['vent'].astype(float)\n",
        "new_df['pluie'] = new_df['pluie'].astype(float)\n",
        "new_df['temp'] = new_df['temp'].astype(float)\n",
        "new_df['anomaly'] = new_df['anomaly'].astype(str)\n",
        "new_df['embeddings'] = new_df['embeddings'].astype(str)\n",
        "\n",
        "# Append data to SQL table\n",
        "new_df.to_sql('sensor_data_with_vectors', con=engine, if_exists='append', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "50236f0e-de7a-4be7-851e-cb24be8ec435",
      "metadata": {},
      "source": [
        "## Dashboard for Monitoring Anomalies over Time\n",
        "\n",
        "### Data Visualization Setup\n",
        "- **Library Imports**: `pandas`, `plotly.express`, and `sqlalchemy`.\n",
        "- **Database Connection**: Establishes connection to the database using `create_engine(connection_url)`.\n",
        "\n",
        "### Data Retrieval and Processing\n",
        "- **SQL Query for Data Fetching**: Retrieves anomaly data from `sensor_data_with_vectors` table, excluding entries with 'none' in the anomaly field.\n",
        "- **Data Preparation**:\n",
        "  - Converts `date` column to datetime format and extracts date part into a new column `date_only`.\n",
        "  - Groups data by `date_only` and `anomaly`, counting occurrences to prepare for visualization.\n",
        "\n",
        "### Plotting Anomalies over Time\n",
        "- **Overall Anomaly Trends**:\n",
        "  - Utilizes `plotly.express` to create a histogram representing anomaly counts over time.\n",
        "  - Each anomaly type is color-coded for distinction.\n",
        "\n",
        "- **City-Specific Anomaly Trends**:\n",
        "  - Further groups data by `city` along with `date_only` and `anomaly`.\n",
        "  - Loops through a predefined list of cities to create separate histograms for each city, showcasing city-specific anomaly trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "58abb86c-a8ae-4e4a-8c48-fc9c5fba67b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine(connection_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "52e860f3-64ff-4a0f-96e4-3cdaa5da88ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = pd.read_sql('select * from sensor_data_with_vectors limit 50000;', engine)\n",
        "df = pd.read_sql(\"select * from sensor_data_with_vectors where anomaly <> 'none' limit 50000;\", engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ae6a5994-fae4-453d-add6-5cfc614fe6c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['date_only'] = df['date'].dt.date\n",
        "# Group data by date and anomaly, then count the instances\n",
        "grouped_df = df.groupby(['date_only', 'anomaly']).size().reset_index(name='counts')\n",
        "\n",
        "# Create line plot with Plotly\n",
        "fig = px.histogram(grouped_df, x='date_only', y='counts', color='anomaly',\n",
        "              title='Anomalies over Time', labels={'date_only': 'Date', 'counts': 'Anomaly Count'})\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6f0fe862-fbb4-4f4b-847a-213e50fdb9bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group data by date, city and anomaly, then count the instances\n",
        "grouped_df = df.groupby(['date_only', 'city', 'anomaly']).size().reset_index(name='counts')\n",
        "\n",
        "# List your cities\n",
        "cities = ['Washington DC', 'New York', 'Los Angeles'] # Add or change according to your DataFrame\n",
        "\n",
        "# Create a separate plot for each city\n",
        "for city in cities:\n",
        "    city_df = grouped_df[grouped_df['city'] == city]\n",
        "    fig = px.histogram(city_df, x='date_only', y='counts', color='anomaly',\n",
        "                  title=f'Anomalies over Time for {city}', labels={'date_only': 'Date', 'counts': 'Anomaly Count'})\n",
        "    fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dd964c7c-b08a-4a68-b2ec-88267d47d78f",
      "metadata": {},
      "source": [
        "## Real-Time Anomaly Detection Dashboard\n",
        "\n",
        "### Environment Setup and Library Imports\n",
        "- **Library Installation**: Installs `tabulate`, `pymysql`, `Ipython`, and `rich` libraries.\n",
        "- **Imports**: Includes libraries such as `time`, `os`, `shutil`, `pymysql`, `rich.console`, `rich.table`, `IPython.display`, `sqlalchemy`, and `pandas`.\n",
        "\n",
        "### Dashboard Function Definition\n",
        "- **Function `display_table_contents`**:\n",
        "  - Establishes a database connection using `create_engine(connection_url)`.\n",
        "  - Executes an SQL query to fetch initial data from `sensor_data_with_vectors` with specific columns (`date`, `vent`, `pluie`, `temp`, `anomaly`).\n",
        "  - Enters a loop to continuously update and display the dashboard.\n",
        "\n",
        "### Dashboard Display Mechanics\n",
        "- **Console and Table Setup**:\n",
        "  - Clears the console output and creates a console instance with `rich.console`.\n",
        "  - Determines the terminal width and sets up a dynamic table layout.\n",
        "  - Adds columns with specific styles and alignments for better readability.\n",
        "\n",
        "- **Data Display and Refresh Loop**:\n",
        "  - Adds the top 50 rows from the fetched data to the table.\n",
        "  - Styles rows based on the anomaly type (e.g., different colors for different anomaly types).\n",
        "  - Refreshes the display every 10 seconds, fetching updated data from the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3c2137ab-acfc-4dfb-9c99-f09d5863b1d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tabulate pymysql Ipython rich --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "83fecd11-e017-44b5-9845-27404b829b9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import pymysql\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "33117fdf-f1d5-4ad2-b957-7ca75586b969",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "def display_table_contents():\n",
        "    # Create a database engine\n",
        "    engine = create_engine(connection_url)\n",
        "\n",
        "    # Execute query to fetch initial table contents\n",
        "    query = 'SELECT date, vent, pluie, temp, anomaly FROM sensor_data_with_vectors ORDER BY date DESC'\n",
        "    table_data = pd.read_sql_query(query, engine)\n",
        "\n",
        "    while True:\n",
        "        # Clear console output\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # Create a console instance\n",
        "        console = Console()\n",
        "\n",
        "        # Get the terminal width\n",
        "        terminal_width = shutil.get_terminal_size().columns\n",
        "\n",
        "        # Print the title with centered alignment\n",
        "        title = \"[bold magenta]REAL TIME ANALYTICS DASHBOARD[/bold magenta]\"\n",
        "        console.print(title.center(terminal_width))\n",
        "\n",
        "        # Create a table instance\n",
        "        table = Table(show_header=True, header_style=\"bold\", box=None)\n",
        "\n",
        "        # Add columns to the table\n",
        "        table.add_column(\"Date\", justify=\"center\", style=\"cyan\", width=terminal_width // 5 + 5)\n",
        "        table.add_column(\"Vent\", justify=\"center\", style=\"magenta\", width=terminal_width // 5)\n",
        "        table.add_column(\"Pluie\", justify=\"center\", style=\"yellow\", width=terminal_width // 5)\n",
        "        table.add_column(\"Temp\", justify=\"center\", style=\"green\", width=terminal_width // 5)\n",
        "        table.add_column(\"Anomaly\", justify=\"center\", width=terminal_width // 5)\n",
        "\n",
        "        # Add rows to the table\n",
        "        for row in table_data.head(50).itertuples(index=False):\n",
        "            # Convert datetime to string before adding to the table\n",
        "            formatted_row = [str(cell) for cell in row]\n",
        "\n",
        "            # Check the anomaly type\n",
        "            anomaly_type = formatted_row[4]\n",
        "\n",
        "            # Determine the style based on the anomaly type\n",
        "            if anomaly_type == 'pluie':\n",
        "                style = \"bold blue\"\n",
        "            elif anomaly_type == 'vent':\n",
        "                style = \"bold magenta\"\n",
        "            elif anomaly_type == 'temp':\n",
        "                style = \"bold green\"\n",
        "            else:\n",
        "                style = \"\"\n",
        "\n",
        "            # Add the row with the appropriate style\n",
        "            table.add_row(*formatted_row, style=style)\n",
        "\n",
        "        # Print the table\n",
        "        console.print(table)\n",
        "\n",
        "        # Wait for 30 seconds before refreshing\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Execute query to fetch updated table contents\n",
        "        updated_data = pd.read_sql_query(query, engine)\n",
        "\n",
        "        # Update the table_data with the fetched data\n",
        "        table_data = updated_data\n",
        "\n",
        "# Call the function to start displaying the table contents\n",
        "display_table_contents()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0e71ca-601d-4a30-8a0c-fc016e473dd7",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
