{
  "cells": [
    {
      "id": "d119cd5f",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(255, 182, 176, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/radar.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Vector Search with Kai</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Search with Kai"
      ],
      "id": "0d0e07b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we load a dataset into a collection, create a vector index and perform vector searches using Kai in a way that is compatible with MongoDB clients and applications"
      ],
      "id": "866a5240"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets --quiet"
      ],
      "id": "62d44fb0"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import time\n",
        "import concurrent.futures\n",
        "import datasets\n",
        "from pymongo import MongoClient\n",
        "from datasets import load_dataset\n",
        "from bson import json_util"
      ],
      "id": "884be378"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Initializing a pymongo client"
      ],
      "id": "50e5eb0e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_database = %sql SELECT DATABASE() as CurrentDatabase\n",
        "DB = current_database[0][0]\n",
        "COLLECTION = 'wiki_embeddings'"
      ],
      "id": "69eb1b7b"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the environment variable that holds the kai endpoint\n",
        "client = MongoClient(connection_url_kai)\n",
        "collection = client[DB][COLLECTION]"
      ],
      "id": "2ad21ed8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create a collection and load the dataset"
      ],
      "id": "9e2264ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is recommended that you create a collection with the embedding field as a top level column for optimized utilization of storage. The name of the column should be the name of the field holding the embedding"
      ],
      "id": "c2cf8565"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "client[DB].create_collection(COLLECTION,\n",
        "  columns=[{ 'id': \"emb\", 'type': \"VECTOR(768) NOT NULL\" }],\n",
        ");"
      ],
      "id": "9594856b"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the \"wikipedia-22-12-simple-embeddings\" dataset from Hugging Face\n",
        "dataset = load_dataset(\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\")"
      ],
      "id": "e9ef7506"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "DB_SIZE = 50000 #Currently loading 50k documents to the collection, can go to a max of 485,859 for this dataset\n",
        "insert_data = []\n",
        "insert_count = 0\n",
        "# Iterate through the dataset and prepare the documents for insertion\n",
        "# The script below ingests 1000 records into the database at a time\n",
        "for item in dataset:\n",
        "    if insert_count >= DB_SIZE:\n",
        "        break\n",
        "    # Convert the dataset item to MongoDB document format\n",
        "    doc_item = json_util.loads(json_util.dumps(item))\n",
        "    insert_data.append(doc_item)\n",
        "\n",
        "    # Insert in batches of 1000 documents\n",
        "    if len(insert_data) == 1000:\n",
        "        collection.insert_many(insert_data)\n",
        "        insert_count += 1000\n",
        "        print(f\"{insert_count} of {DB_SIZE} records ingested\")\n",
        "        insert_data = []\n",
        "\n",
        "\n",
        "# Insert any remaining documents\n",
        "if len(insert_data) > 0:\n",
        "    collection.insert_many(insert_data)\n",
        "    print(\"Data Ingested\")"
      ],
      "id": "647d6232"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A sample document from the collection"
      ],
      "id": "0e512df2"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_doc = collection.find_one()\n",
        "pprint.pprint(sample_doc, compact=True)"
      ],
      "id": "274582aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Create a vector Index"
      ],
      "id": "adbdd29f"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "client[DB].command({\n",
        "    'createIndexes': COLLECTION,\n",
        "    'indexes': [{\n",
        "        'key': {'emb': 'vector'},\n",
        "        'name': 'vector_index',\n",
        "        'kaiSearchOptions': {\"index_type\":\"AUTO\", \"metric_type\": \"EUCLIDEAN_DISTANCE\", \"dimensions\": 768}\n",
        "    }],\n",
        "})"
      ],
      "id": "aa1945d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting the query embedding from the sample_doc selected above"
      ],
      "id": "3acb0065"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input vector\n",
        "query_vector = sample_doc['emb']"
      ],
      "id": "808f1cbd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Perform a vector search"
      ],
      "id": "6d868667"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_kai_search(query_vector):\n",
        "    pipeline = [\n",
        "        {\n",
        "            '$vectorSearch': {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"path\": \"emb\",\n",
        "                \"queryVector\": query_vector,\n",
        "                \"numCandidates\": 20,\n",
        "                \"limit\": 3,\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            '$project': {\n",
        "               '_id':1,\n",
        "               'text': 1,\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)"
      ],
      "id": "702698e1"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_kai_search(query_vector)"
      ],
      "id": "ccbd0433"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running concurrent vector search queries"
      ],
      "id": "79a09da6"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_concurrent_queries = 250\n",
        "start_time = time.time()\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent_queries) as executor:\n",
        "    futures = [executor.submit(execute_kai_search, query_vector) for _ in range(num_concurrent_queries)]\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Executed {num_concurrent_queries} concurrent queries.\")\n",
        "print(f\"Total execution time: {end_time - start_time} seconds\")\n",
        "\n",
        "for f in futures:\n",
        "    if f.exception() is not None:\n",
        "        print(f.exception())\n",
        "failed_count = sum(1 for f in futures if f.exception() is not None)\n",
        "print(f\"Failed queries: {failed_count}\")"
      ],
      "id": "15224548"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows the Kai can create vector indexes instantaneously and perform a large number of concurrent vector search queries surpassing MongoDB Atlas Vector Search capabilities"
      ],
      "id": "fdb6d999"
    },
    {
      "id": "8403dbcd",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
