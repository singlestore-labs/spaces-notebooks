{
  "cells": [
    {
      "id": "67480459",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(255, 167, 103, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/crystal-ball.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Semantic Search with OpenAI QA</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "id": "09632a57",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Note</b></p>\n",
        "        <p>This notebook can be run on a Free Starter Workspace. To create a Free Starter Workspace navigate to <tt>Start</tt> using the left nav. You can also use your existing Standard or Premium workspace with this Notebook.</p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this Notebook you will use a combination of Semantic Search and a Large Langauge Model (LLM) to build a basic Retrieval Augmented Generation (RAG) application. For a great introduction into what RAG is, please read [A Beginner's Guide to Retrieval Augmented Generation (RAG)](https://www.singlestore.com/blog/a-guide-to-retrieval-augmented-generation-rag/).\n",
        "## Prerequisites for interacting with ChatGPT"
      ],
      "id": "5887cda1"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install OpenAI package\n",
        "\n",
        "Let's start by installing the [openai](https://platform.openai.com/docs/api-reference?lang=python) Python package."
      ],
      "id": "d55d2aa8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai==1.3.3 --quiet"
      ],
      "id": "297675eb"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to ChatGPT and display the response"
      ],
      "id": "37988f8c"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "GPT_MODEL = \"gpt-3.5-turbo\""
      ],
      "id": "60318fb1"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will need an OpenAI API key in order to use the the `openai` Python library."
      ],
      "id": "44c80931"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n",
        "\n",
        "client = openai.OpenAI()"
      ],
      "id": "603fbac4"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the connection."
      ],
      "id": "a29c90d9"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=GPT_MODEL,\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the gold medal for curling in Olymics 2022?\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "id": "23d11168"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get the data about Winter Olympics and provide the information to ChatGPT as context"
      ],
      "id": "2363818e"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and import libraries"
      ],
      "id": "98ab17e2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tabulate tiktoken wget --quiet"
      ],
      "id": "9a18969c"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import wget"
      ],
      "id": "a4a0ba11"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch the CSV data and read it into a DataFrame"
      ],
      "id": "ce88a189"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download pre-chunked text and pre-computed embeddings. This file is ~200 MB, so may take a minute depending on your connection speed."
      ],
      "id": "ee0738ce"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_url = \"https://cdn.openai.com/API/examples/data/winter_olympics_2022.csv\"\n",
        "embeddings_path = \"winter_olympics_2022.csv\"\n",
        "\n",
        "if not os.path.exists(embeddings_path):\n",
        "    wget.download(embeddings_url, embeddings_path)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"File already exists in the local file system.\")"
      ],
      "id": "bdfd15f0"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we are using the `converters=` parameter of the `pd.read_csv` function to convert the JSON\n",
        "array in the CSV file to numpy arrays."
      ],
      "id": "66a2c515"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def json_to_numpy_array(x: str | None) -> np.ndarray | None:\n",
        "    \"\"\"Convert JSON array string into numpy array.\"\"\"\n",
        "    return np.array(json.loads(x)) if x else None\n",
        "\n",
        "df = pd.read_csv(embeddings_path, converters=dict(embedding=json_to_numpy_array))\n",
        "df"
      ],
      "id": "1956ecc7"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info(show_counts=True)"
      ],
      "id": "7b0569fd"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Set up the database"
      ],
      "id": "e5f60bd0"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> If you have a Free Starter Workspace deployed already, select the database from drop-down menu at the top of this notebook. It updates the <tt>connection_url</tt> to connect to that database.</p>\n",
        "    </div>\n",
        "</div>"
      ],
      "id": "43947d3d"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the database."
      ],
      "id": "3f1baf5b"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_tier_check = %sql show variables like 'is_shared_tier'\n",
        "if not shared_tier_check or shared_tier_check[0][1] == 'OFF':\n",
        "    %sql DROP DATABASE IF EXISTS winter_wikipedia;\n",
        "    %sql CREATE DATABASE winter_wikipedia;"
      ],
      "id": "4f789d23"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p>Make sure to select the <tt>winter_wikipedia</tt> database from the drop-down menu at the top of this notebook.\n",
        "        It updates the <tt>connection_url</tt> which is used by the <tt>%%sql</tt> magic command and SQLAlchemy to make connections to the selected database.</p>\n",
        "    </div>\n",
        "</div>"
      ],
      "id": "a5fd7f34"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "CREATE TABLE IF NOT EXISTS winter_olympics_2022 /* Creating table for sample data. */(\n",
        "    id INT PRIMARY KEY,\n",
        "    text TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "    embedding BLOB\n",
        ");"
      ],
      "id": "a6cbcb7e"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Populate the table with our DataFrame"
      ],
      "id": "4dedda5c"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a SQLAlchemy connection."
      ],
      "id": "cae0a395"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import singlestoredb as s2\n",
        "\n",
        "conn = s2.create_engine().connect()"
      ],
      "id": "dba9a833"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the `to_sql` method of the DataFrame to upload the data to the requested table."
      ],
      "id": "490ccfe3"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_sql('winter_olympics_2022', con=conn, index=True, index_label='id', if_exists='append', chunksize=1000)"
      ],
      "id": "1360d505"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Do a semantic search with the same question from above and use the response to send to OpenAI again"
      ],
      "id": "8ea4a8eb"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlalchemy as sa\n",
        "\n",
        "\n",
        "def get_embedding(text: str, model: str = 'text-embedding-ada-002') -> str:\n",
        "    \"\"\"Return the embeddings.\"\"\"\n",
        "    return [x.embedding for x in client.embeddings.create(input=[text], model=model).data][0]\n",
        "\n",
        "\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    table_name: str,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int=100,\n",
        ") -> tuple:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "\n",
        "    # Get the embedding of the query.\n",
        "    query_embedding_response = get_embedding(query, EMBEDDING_MODEL)\n",
        "\n",
        "    # Create the SQL statement.\n",
        "    stmt = sa.text(f\"\"\"\n",
        "        SELECT\n",
        "            text,\n",
        "            DOT_PRODUCT_F64(JSON_ARRAY_PACK_F64(:embedding), embedding) AS score\n",
        "        FROM {table_name}\n",
        "        ORDER BY score DESC\n",
        "        LIMIT :limit\n",
        "    \"\"\")\n",
        "\n",
        "    # Execute the SQL statement.\n",
        "    results = conn.execute(stmt, dict(embedding=json.dumps(query_embedding_response), limit=top_n))\n",
        "\n",
        "    strings = []\n",
        "    relatednesses = []\n",
        "\n",
        "    for row in results:\n",
        "        strings.append(row[0])\n",
        "        relatednesses.append(row[1])\n",
        "\n",
        "    # Return the results.\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "id": "a147f99c"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "strings, relatednesses = strings_ranked_by_relatedness(\n",
        "    \"curling gold medal\",\n",
        "    df,\n",
        "    \"winter_olympics_2022\",\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "for string, relatedness in zip(strings, relatednesses):\n",
        "    print(f\"{relatedness=:.3f}\")\n",
        "    print(tabulate([[string]], headers=['Result'], tablefmt='fancy_grid'))\n",
        "    print('\\n\\n')"
      ],
      "id": "a8edab89"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "\n",
        "def num_tokens(text: str, model: str=GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from SingleStoreDB.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df, \"winter_olympics_2022\")\n",
        "    introduction = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame=df,\n",
        "    model: str=GPT_MODEL,\n",
        "    token_budget: int=4096 - 500,\n",
        "    print_message: bool=False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a table of relevant texts and embeddings in SingleStoreDB.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    if print_message:\n",
        "        print(message)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message"
      ],
      "id": "231acb2b"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ask('Who won the gold medal for curling in Olymics 2022?'))"
      ],
      "id": "cda561e4"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up"
      ],
      "id": "77dde827"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> If you created a new database in your Standard or Premium Workspace, you can drop the database by running the cell below. Note: this will not drop your database for Free Starter Workspaces. To drop a Free Starter Workspace, terminate the Workspace using the UI. </p>\n",
        "    </div>\n",
        "</div>"
      ],
      "id": "750bbd14"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_tier_check = %sql show variables like 'is_shared_tier'\n",
        "if not shared_tier_check or shared_tier_check[0][1] == 'OFF':\n",
        "    %sql DROP DATABASE IF EXISTS winter_wikipedia;"
      ],
      "id": "dd07be8d"
    },
    {
      "id": "2a95146c",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
