{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b05a4dfa-1763-431d-9027-75c783712e85",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(255, 167, 103, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/crystal-ball.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Semantic Search with OpenAI QA</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7002feac",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Note</b></p>\n",
        "        <p> This notebook can be run on a Free Starter Workspace. To create a Free Starter Workspace navigate to <tt>Start</tt> using the left nav. You can also use your existing Standard or Premium workspace with this Notebook. </p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f801cd94-180c-4dea-b85c-67659aad0ea6",
      "metadata": {},
      "source": [
        "In this Notebook you will use a combination of Semantic Search and a Large Langauge Model (LLM) to build a basic Retrieval Augmented Generation (RAG) application. For a great introduction into what RAG is, please read [A Beginner's Guide to Retrieval Augmented Generation (RAG)](https://www.singlestore.com/blog/a-guide-to-retrieval-augmented-generation-rag/).\n",
        "## Prerequisites for interacting with ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df04d713-f330-4335-9837-3ab79eb552d6",
      "metadata": {},
      "source": [
        "### Install OpenAI package\n",
        "\n",
        "Let's start by installing the [openai](https://platform.openai.com/docs/api-reference?lang=python) Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7690dc5e-93f4-477d-87b2-db35da95fb65",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai==1.3.3 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62bd45fa-daac-4d71-ab76-be014ddd3a32",
      "metadata": {},
      "source": [
        "### Connect to ChatGPT and display the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9c4378f4-02d4-4c19-a512-f3eed0a9cb88",
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "GPT_MODEL = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c244aa25-f548-47b2-8942-991552dc0ca1",
      "metadata": {},
      "source": [
        "You will need an OpenAI API key in order to use the the `openai` Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c2114aea-e137-407e-8e05-bf5688594a98",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n",
        "\n",
        "client = openai.OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0663c6f2-7741-4966-aea8-d5629e4a1cd4",
      "metadata": {},
      "source": [
        "Test the connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "244f1d45-db89-489e-aad0-153a652d59f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=GPT_MODEL,\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the gold medal for curling in Olymics 2022?\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d287b813-2885-4b22-a431-03c6b4eab058",
      "metadata": {},
      "source": [
        "# Get the data about Winter Olympics and provide the information to ChatGPT as context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682326b6-a475-4d79-828d-951780a6fb96",
      "metadata": {},
      "source": [
        "## 1. Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b5ef7c8a-7ab0-473d-b944-8cdbfe4918d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tabulate tiktoken wget --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a01f2552-cde3-49a4-8205-72b8fa8260c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import wget"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7aee40-4774-4ef1-b700-a83f9fed4fbb",
      "metadata": {},
      "source": [
        "## 2. Fetch the CSV data and read it into a DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05fcb9a8-2290-4507-aad1-a3002cab0ba6",
      "metadata": {},
      "source": [
        "Download pre-chunked text and pre-computed embeddings. This file is ~200 MB, so may take a minute depending on your connection speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bef09ede-40b1-40f3-9966-f68d4b025fbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_url = \"https://cdn.openai.com/API/examples/data/winter_olympics_2022.csv\"\n",
        "embeddings_path = \"winter_olympics_2022.csv\"\n",
        "\n",
        "if not os.path.exists(embeddings_path):\n",
        "    wget.download(embeddings_url, embeddings_path)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"File already exists in the local file system.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1faf22b7-5b99-4a9b-a88a-24acb16d133e",
      "metadata": {},
      "source": [
        "Here we are using the `converters=` parameter of the `pd.read_csv` function to convert the JSON\n",
        "array in the CSV file to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6e0669bf-9070-42bd-a561-f6729f9203a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def json_to_numpy_array(x: str | None) -> np.ndarray | None:\n",
        "    \"\"\"Convert JSON array string into numpy array.\"\"\"\n",
        "    return np.array(json.loads(x)) if x else None\n",
        "\n",
        "df = pd.read_csv(embeddings_path, converters=dict(embedding=json_to_numpy_array))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bcf14e64-982e-465b-8e2b-6239650b2f51",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info(show_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb523f8c-78b2-4a75-be15-52d29fac0fff",
      "metadata": {},
      "source": [
        "## 3. Set up the database"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117daa30",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> If you have a Free Starter Workspace deployed already, select the database from drop-down menu at the top of this notebook. It updates the <tt>connection_url</tt> to connect to that database.</p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca811e5f-6dcd-471b-a4de-03eab42acf4f",
      "metadata": {},
      "source": [
        "Create the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ca33b770-95f9-47ae-9509-5dd98c331037",
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_tier_check = %sql show variables like 'is_shared_tier'\n",
        "if not shared_tier_check or shared_tier_check[0][1] == 'OFF':\n",
        "    %sql DROP DATABASE IF EXISTS winter_wikipedia;\n",
        "    %sql CREATE DATABASE winter_wikipedia;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393e0d4a-8020-447e-b0ae-aa4199b1a016",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p>Make sure to select the <tt>winter_wikipedia</tt> database from the drop-down menu at the top of this notebook.\n",
        "        It updates the <tt>connection_url</tt> which is used by the <tt>%%sql</tt> magic command and SQLAlchemy to make connections to the selected database.</p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "011afdb9-cbd4-42af-8121-a8928d5c8432",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "CREATE TABLE IF NOT EXISTS winter_olympics_2022 (\n",
        "    id INT PRIMARY KEY,\n",
        "    text TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "    embedding BLOB\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7ab530-4f55-482f-8e4c-475df06fe9b3",
      "metadata": {},
      "source": [
        "## 4. Populate the table with our DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51be94b1-9901-499c-a364-c85782239e2a",
      "metadata": {},
      "source": [
        "Create a SQLAlchemy connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b8722e7e-211e-44a6-b512-eee1719c2879",
      "metadata": {},
      "outputs": [],
      "source": [
        "import singlestoredb as s2\n",
        "\n",
        "conn = s2.create_engine().connect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce8c4c5-f389-4d0d-b434-6cd628343688",
      "metadata": {},
      "source": [
        "Use the `to_sql` method of the DataFrame to upload the data to the requested table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8dd44f53-f6ec-4009-8d1d-95209d704eec",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_sql('winter_olympics_2022', con=conn, index=True, index_label='id', if_exists='append', chunksize=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d4602c-bfec-4819-904b-4d376b920e44",
      "metadata": {},
      "source": [
        "## 5. Do a semantic search with the same question from above and use the response to send to OpenAI again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e8560501-ff5e-4727-8d07-99f2173a3d62",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlalchemy as sa\n",
        "\n",
        "\n",
        "def get_embedding(text: str, model: str = 'text-embedding-ada-002') -> str:\n",
        "    \"\"\"Return the embeddings.\"\"\"\n",
        "    return [x.embedding for x in client.embeddings.create(input=[text], model=model).data][0]\n",
        "\n",
        "\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    table_name: str,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int=100,\n",
        ") -> tuple:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "\n",
        "    # Get the embedding of the query.\n",
        "    query_embedding_response = get_embedding(query, EMBEDDING_MODEL)\n",
        "\n",
        "    # Create the SQL statement.\n",
        "    stmt = sa.text(f\"\"\"\n",
        "        SELECT\n",
        "            text,\n",
        "            DOT_PRODUCT_F64(JSON_ARRAY_PACK_F64(:embedding), embedding) AS score\n",
        "        FROM {table_name}\n",
        "        ORDER BY score DESC\n",
        "        LIMIT :limit\n",
        "    \"\"\")\n",
        "\n",
        "    # Execute the SQL statement.\n",
        "    results = conn.execute(stmt, dict(embedding=json.dumps(query_embedding_response), limit=top_n))\n",
        "\n",
        "    strings = []\n",
        "    relatednesses = []\n",
        "\n",
        "    for row in results:\n",
        "        strings.append(row[0])\n",
        "        relatednesses.append(row[1])\n",
        "\n",
        "    # Return the results.\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a1b27188-409c-4e43-8065-5448f40e1986",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "strings, relatednesses = strings_ranked_by_relatedness(\n",
        "    \"curling gold medal\",\n",
        "    df,\n",
        "    \"winter_olympics_2022\",\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "for string, relatedness in zip(strings, relatednesses):\n",
        "    print(f\"{relatedness=:.3f}\")\n",
        "    print(tabulate([[string]], headers=['Result'], tablefmt='fancy_grid'))\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ce7411d2-6f3c-408b-996b-2ec34cad5d7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "\n",
        "def num_tokens(text: str, model: str=GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from SingleStoreDB.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df, \"winter_olympics_2022\")\n",
        "    introduction = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame=df,\n",
        "    model: str=GPT_MODEL,\n",
        "    token_budget: int=4096 - 500,\n",
        "    print_message: bool=False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a table of relevant texts and embeddings in SingleStoreDB.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    if print_message:\n",
        "        print(message)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "46a21d10-88bb-4cfb-b68d-b4a5b12ba1f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ask('Who won the gold medal for curling in Olymics 2022?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0935c8cb-a397-4892-b0ef-5b5ddee2b82a",
      "metadata": {},
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1202b364",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p> If you created a new database in your Standard or Premium Workspace, you can drop the database by running the cell below. Note: this will not drop your database for Free Starter Workspaces. To drop a Free Starter Workspace, terminate the Workspace using the UI. </p>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5f448b09-8855-4dc7-984e-7d5dfdb68fc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_tier_check = %sql show variables like 'is_shared_tier'\n",
        "if not shared_tier_check or shared_tier_check[0][1] == 'OFF':\n",
        "    %sql DROP DATABASE IF EXISTS winter_wikipedia;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cca5fc-9cf5-474b-820f-440255193976",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
