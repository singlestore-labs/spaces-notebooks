{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f30fc302-c915-4c1f-94a2-0b237798086e",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(235, 249, 245, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/database.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Database Performance Troubleshoot Notebook</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table style=\"border: 0; border-spacing: 0; width: 100%; background-color: #03010D\"><tr>\n",
        "    <td style=\"padding: 0; margin: 0; background-color: #03010D; width: 33%; text-align: center\"><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-vertical.png\" style=\"height: 200px;\"/></td>\n",
        "    <td style=\"padding: 0; margin: 0; width: 66%; background-color: #03010D; text-align: right\"><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-jupyter.png\" style=\"height: 250px\"/></td>\n",
        "</tr></table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Intro\n",
        "\n",
        "<p class=\"has-text-justified\">\n",
        "    Introducing a powerful Python script designed to ease performance analysis tasks for database management.\n",
        "</p>\n",
        "\n",
        "<ol>\n",
        "    <li>This script loads query information from csv file exposed on public URL</li>\n",
        "    <li>Executes SQL queries against selected database</li>\n",
        "    <li>Exports results to searchable html tables and uploads archive of generated html files with index into stage area</li>\n",
        "    <li>Handles Stage Area operations using singlestore python client which uses SingleStore Management API</li>\n",
        "    <li>Simplifying complex tasks, this script is essential for streamlining workflows for administrators and developers alike</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "## What you will learn in this notebook:\n",
        "\n",
        "1. How to read a csv and load data into pandas dataframes[Python] Download DB_PERFORMANCE_TROUBLESHOOT_QUERIES.csv file from url\n",
        "2. Execute queries and export result into html files [Python]\n",
        "4. Use of SingleStore client for db operations and stage area [Python]\n",
        "\n",
        "\n",
        "## What benefits do you get out of using the notebook.\n",
        "\n",
        "1. User will be able to run most used performance checks\n",
        "2. Results are exported into HTML for better view\n",
        "3. Along with analysis of known scenarios, script also provides background and possible actions to take\n",
        "\n",
        "\n",
        "\n",
        "## Questions?\n",
        "\n",
        "Reach out to us through our [forum](https://www.singlestore.com/forum)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-requisites\n",
        "\n",
        "We will need below parameters to proceed.\n",
        "\n",
        "\n",
        "\n",
        "<ol type=\"A\">\n",
        "    <li>SingleStore Management API KEY. Follow this <a href=\"https://docs.singlestore.com/cloud/reference/management-api/\">link</a> for API Key </li>\n",
        "    <li>Directory Path of Stage Area ( Target location to upload archive )</li>\n",
        "    <li>URL to download csv file</li>\n",
        "    <li>URL of result template directory</li>\n",
        "</ol>\n",
        "\n",
        "<p>\n",
        "    Note: You may use the\n",
        "    <ul>\n",
        "        <li><a href=\"https://github.com/singlestore-labs/spaces-notebooks/tree/master/notebooks/performance-troubleshooting/assets/DB_PERFORMANCE_TROUBLESHOOT_QUERIES.csv\">DB_PERFORMANCE_TROUBLESHOOT_QUERIES.csv</a> as template to add up your queries.</li>\n",
        "    <li><a href=\"https://github.com/singlestore-labs/spaces-notebooks/tree/master/notebooks/performance-troubleshooting/assets/templates\">templates</a> as templates  for results</li>\n",
        "    </ul>\n",
        "</p>\n",
        "<p>\n",
        "    For simplicity of demo, here we are using a public accessible URL, you have to adapt access pattern to suit your needs.\n",
        "</p>\n",
        "\n",
        "CSV File structure\n",
        "\n",
        "<table class=\"table is-bordered is-narrow\">\n",
        "<th>\n",
        "     <td>QueryID</td>\n",
        "     <td>QueryName</td>\n",
        "     <td>QueryTxt</td>\n",
        "</th>\n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install openpyxl jsonpath_ng sql_metadata"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import singlestoredb as s2\n",
        "\n",
        "from pathlib import Path\n",
        "from urllib.request import urlopen\n",
        "from jsonpath_ng import parse\n",
        "from sql_metadata import Parser\n",
        "from urllib.error import HTTPError\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_data_url = \"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/notebooks/performance-troubleshooting/assets/DB_PERFORMANCE_TROUBLESHOOT_QUERIES.csv\"\n",
        "template_url_base = \"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/notebooks/performance-troubleshooting/assets/templates/\"\n",
        "\n",
        "stage_folder_path = 'DBPERF-REPORT'\n",
        "\n",
        "my_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "local_output_dir_suffix = '_' + my_timestamp + '_PERF_REPORT'\n",
        "\n",
        "empty_result_table = '<p class=\"mb-3 mt-3\" style=\"text-align:center;color:blue;\">No Matching Records Found</p>'\n",
        "result_table_html_classes = 'table table-striped table-bordered table-responsive my-2 px-2'\n",
        "\n",
        "WORKGROUP_ID = os.getenv('SINGLESTOREDB_WORKSPACE_GROUP')\n",
        "\n",
        "HEADERS = [\"PLAN_ID\", \"DATABASE_NAME\", \"SQL_QUERY\",\"SUGGESTION\",\"CMP_EXP\",\n",
        "           \"LEFT_TABLE\", \"LEFT_COLUMN\", \"LEFT_TYPE\", \"LEFT_TABLE_ROW_COUNT\",\n",
        "           \"RIGHT_TABLE\", \"RIGHT_COLUMN\", \"RIGHT_TYPE\", \"RIGHT_TABLE_ROW_COUNT\" ]\n",
        "\n",
        "s2_workgroup_stage = None\n",
        "s2_workspace_name = None\n",
        "\n",
        "row_count_parser = parse(\"$.*.rowcount\")\n",
        "table_row_count_cache = {}\n",
        "\n",
        "MISMATCH_DATATYPE_ROWCOUNT_COMP_QUERY = \"\"\"\n",
        "WITH mismatched_comp\n",
        "     AS (SELECT plan_id,\n",
        "                database_name,\n",
        "                table_col AS mis_cmp\n",
        "         FROM   information_schema.plancache\n",
        "                cross join TABLE(JSON_TO_ARRAY(\n",
        "                                 plan_info :: type_mismatched_comparisons)\n",
        "                           ) HAVING plan_warnings like '%%Comparisons between mismatched datatypes%%' ),\n",
        "     mismatche_cmp_2\n",
        "     AS (SELECT plan_id,\n",
        "                database_name,\n",
        "                mis_cmp ::$ comparison_expression AS cmp_exp,\n",
        "                mis_cmp ::$ left_type             AS left_type,\n",
        "                mis_cmp ::$ right_type            AS right_type\n",
        "         FROM   mismatched_comp),\n",
        "     plan_tbl_row_counts\n",
        "     AS (SELECT plan_id,\n",
        "                database_name,\n",
        "                optimizer_notes ::$ table_row_counts AS tbl_row_counts,\n",
        "                query_text\n",
        "         FROM   information_schema.plancache)\n",
        "SELECT m2.plan_id as PLAN_ID,\n",
        "       m2.database_name as DATABASE_NAME ,\n",
        "       m2.cmp_exp as CMP_EXP,\n",
        "       m2.left_type as LEFT_TYPE,\n",
        "       m2.right_type as RIGHT_TYPE,\n",
        "       p2.tbl_row_counts as TBL_ROW_COUNTS,\n",
        "       p2.query_text AS SQL_QUERY\n",
        "FROM   mismatche_cmp_2 m2,\n",
        "       plan_tbl_row_counts p2\n",
        "WHERE  m2.plan_id = p2.plan_id\n",
        "       AND m2.database_name = p2.database_name ;\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_logging_enabled(enabled):\n",
        "    \"\"\"\n",
        "    Set the logging level based on the enabled flag.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    enabled : bool\n",
        "        True to enable logging, False to disable it.\n",
        "    \"\"\"\n",
        "    if enabled:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "    else:\n",
        "        logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "set_logging_enabled(False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note** To enable logs\n",
        "\n",
        " - Modify 'set_logging_enabled(False)' to 'set_logging_enabled(True)' in code below"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions to display various alerts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_warn(warn_msg):\n",
        "    \"\"\"\n",
        "    Display a warning message in a formatted HTML alert box.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    warn_msg : str\n",
        "        The warning message to display.\n",
        "    \"\"\"\n",
        "    display(HTML(f'''<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Action Required</b></p>\n",
        "        <p>{warn_msg}</p>\n",
        "    </div>\n",
        "</div>'''))\n",
        "\n",
        "\n",
        "def show_error(error_msg):\n",
        "    \"\"\"\n",
        "    Display an error message in a formatted HTML alert box.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    error_msg : str\n",
        "        The error message to display.\n",
        "    \"\"\"\n",
        "    display(HTML(f'''<div class=\"alert alert-block alert-danger\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-triangle\"></b>\n",
        "    <div>\n",
        "        <p><b>Error</b></p>\n",
        "        <p>{error_msg}</p>\n",
        "    </div>\n",
        "</div>'''))\n",
        "\n",
        "\n",
        "def show_success(success_msg):\n",
        "    \"\"\"\n",
        "    Display a success message in a formatted HTML alert box.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    success_msg : str\n",
        "        The success message to display.\n",
        "    \"\"\"\n",
        "    display(HTML(f'''<div class=\"alert alert-block alert-success\">\n",
        "    <b class=\"fa fa-solid fa-check-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Success</b></p>\n",
        "        <p>{success_msg}</p>\n",
        "    </div>\n",
        "</div>'''))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utility functions handling db connection and archiving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_query(dbcon, query_txt):\n",
        "    \"\"\"\n",
        "    Execute a SQL query on the specified database connection.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dbcon : connection\n",
        "        The database connection object.\n",
        "    query_txt : str\n",
        "        The SQL query to execute.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of rows returned by the query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with dbcon.cursor() as cur:\n",
        "            cur.execute(query_txt)\n",
        "        return cur.fetchall()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to execute query: {e}\")\n",
        "        raise Exception('Failed to execute query')\n",
        "\n",
        "\n",
        "def make_tarfile(output_filename, source_dir):\n",
        "    \"\"\"\n",
        "    Create a tar.gz archive of a directory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_filename : str\n",
        "        The name of the output archive file.\n",
        "    source_dir : str\n",
        "        The path to the directory to archive.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the archive was created successfully, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "            tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
        "        time.sleep(2)\n",
        "        file_stats = os.stat(output_filename)\n",
        "        logging.info(f'{output_filename} has size {(file_stats.st_size / (1024 * 1024))} mb')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f'Failed to create archive: {e}')\n",
        "        raise Exception(f'Failed to create archive: {e}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utility functions handling HTML generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def copy_index_file(p_local_dir_path, p_templ_base_url):\n",
        "    \"\"\"\n",
        "    Copy the index file to the local directory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p_local_dir_path : str\n",
        "        The local directory path.\n",
        "    p_templ_base_url : str\n",
        "        The base URL for the template.\n",
        "\n",
        "    \"\"\"\n",
        "    index_file = f'{p_local_dir_path}/index.html'\n",
        "    # Fetch the content of the index template from a URL\n",
        "    index_file_content = fetch_url_content(p_templ_base_url + 'index.template.html')\n",
        "    # Write the index file content to a local file\n",
        "    with open(index_file, 'w') as writer:\n",
        "        writer.write(str(index_file_content))\n",
        "    logging.info('Index Page are generated')\n",
        "\n",
        "\n",
        "def generate_html_list(links):\n",
        "    \"\"\"\n",
        "    Generate an HTML ordered list from a comma-separated list of links.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    links : str\n",
        "        A comma-separated list of links.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The HTML formatted ordered list.\n",
        "    \"\"\"\n",
        "    if 'nan' == links:\n",
        "        return ''\n",
        "\n",
        "    html_list = '<ol>'\n",
        "    for item in links.split(','):\n",
        "        html_list += f'<li><a href=\"{item}\">{item}</a></li>'\n",
        "    html_list += '</ol>'\n",
        "    return html_list\n",
        "\n",
        "def generate_stage_link(title, stg_path, curr_file_path):\n",
        "    \"\"\"\n",
        "    Generate an HTML link to a stage area.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stg_path : str\n",
        "        The path to the stage area.\n",
        "    curr_file_path : str\n",
        "        The current file path.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The HTML formatted link.\n",
        "    \"\"\"\n",
        "    url = f\"https://portal.singlestore.com/organizations/{os.environ['SINGLESTOREDB_ORGANIZATION']}/workspaces/{os.environ['SINGLESTOREDB_WORKSPACE_GROUP']}#stage/{stg_path}\"\n",
        "    return f\"\"\"<div style=\\\"text-align:center;margin-top:5px; margin-bottom:5px;\\\">\n",
        "                {title}\n",
        "                 STAGE Link &nbsp;&nbsp;&nbsp;&nbsp; <a href='{url}'> {curr_file_path} </a>\n",
        "               </div>\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function loading query data in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_url_content(url):\n",
        "    \"\"\"\n",
        "    Fetch the content of a URL.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        The URL to fetch.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The content of the URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with urlopen(url) as response:\n",
        "            if response.status == 200:\n",
        "                my_bytes = response.read()\n",
        "                file_content = my_bytes.decode(\"utf8\")\n",
        "                return file_content\n",
        "    except HTTPError as e:\n",
        "        logging.error(f'Failed to read {url} - HTTP error code: {e.code} reason: {e.reason}')\n",
        "        raise Exception(f'Failed to read {url} - HTTP error code: {e.code} reason: {e.reason}')\n",
        "\n",
        "\n",
        "def load_query_data(url):\n",
        "    \"\"\"\n",
        "    Load CSV data from a URL into a pandas DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        The URL of the CSV file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        The loaded DataFrame.\n",
        "    \"\"\"\n",
        "    csv_file_content = fetch_url_content(url)\n",
        "    csv_df = pd.read_csv(io.StringIO(csv_file_content), sep=\",\",\n",
        "                         dtype={'QueryID': int, 'QueryName': str, 'QueryTxt': str, 'QueryParams': str})\n",
        "    csv_df.sort_values(by=['QueryID'], inplace=True)\n",
        "    return csv_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Stage Path and Create if not exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_stage_area():\n",
        "    \"\"\"\n",
        "    Verify the existence and writability of a stage area.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the stage area is valid, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        global s2_workgroup_stage, s2_workspace_name\n",
        "        my_workspace_mngr = s2.manage_workspaces()\n",
        "        workspace_group = my_workspace_mngr.get_workspace_group(WORKGROUP_ID)\n",
        "        s2_workspace_name = my_workspace_mngr.get_workspace(os.environ['SINGLESTOREDB_WORKSPACE']).name\n",
        "        stage_obj = workspace_group.stage.mkdir(stage_path=stage_folder_path, overwrite=False)\n",
        "        logging.info(\n",
        "            f'Stage Path {stage_folder_path} is ok. Is Directory: {stage_obj.is_dir()}. Is Writeable: {stage_obj.writable}')\n",
        "        if stage_obj.is_dir() and stage_obj.writable:\n",
        "            s2_workgroup_stage = workspace_group.stage\n",
        "            logging.info(f'stage is valid: {s2_workgroup_stage is not None}')\n",
        "            return True\n",
        "        else:\n",
        "            logging.error(f'As provided path is neither directory nor writable.')\n",
        "            return False\n",
        "    except Exception as stage_ex:\n",
        "        logging.error(f'Stage Path Verification Failed. {stage_ex}')\n",
        "        return False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions to analyze data type mismatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_table_row_counts(plan_id,json_data):\n",
        "    \"\"\"\n",
        "    Extract table names and their corresponding rowcounts from a JSON string.\n",
        "\n",
        "    Args:\n",
        "    json_data (str): The JSON string containing table names and rowcounts.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing table names as keys and their rowcounts as values.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not json_data:\n",
        "            logging.warning(f'Plan id: {plan_id}, Optimizer notes does not contain row count element')\n",
        "            return None\n",
        "\n",
        "         # Extract rowcounts for each table\n",
        "        matches = row_count_parser.find(json.loads(json_data))\n",
        "\n",
        "        # Create a dictionary to store table names and rowcounts\n",
        "        table_rowcounts = {}\n",
        "        for match in matches:\n",
        "            # Extract the table name from the JSONPath match\n",
        "            table_name = match.full_path.left.fields[0]\n",
        "            rowcount = match.value\n",
        "            table_rowcounts[table_name] = rowcount\n",
        "\n",
        "        return table_rowcounts\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Raise an error if JSON parsing fails\n",
        "        logging.error(\"Invalid JSON data: \" + str(e))\n",
        "        raise ValueError(\"Invalid JSON data: \" + str(e))\n",
        "\n",
        "\n",
        "def fetch_table_row_counts(database_name, table_name):\n",
        "    \"\"\"\n",
        "    Fetch the row count for a given table from the database, using a cursor.\n",
        "\n",
        "    Args:\n",
        "    cursor: The database cursor object.\n",
        "    database_name (str): The name of the database.\n",
        "    table_name (str): The name of the table.\n",
        "\n",
        "    Returns:\n",
        "    int: The row count of the table, or -1 if the row count cannot be determined.\n",
        "    \"\"\"\n",
        "    global table_row_count_cache\n",
        "\n",
        "    if database_name:\n",
        "        lookup_key = f'{database_name}.{table_name}'\n",
        "        # Check if the row count is already cached\n",
        "        if lookup_key in table_row_count_cache.keys():\n",
        "            return table_row_count_cache[lookup_key]\n",
        "        else:\n",
        "            logging.warning(f'{lookup_key} is missing in cache, will fetch and update cache')\n",
        "            result = None\n",
        "            with s2.connect(results_type='dict').cursor() as cursor:\n",
        "                # Fetch the row count from the database\n",
        "                cursor.execute(f\"select sum(rows) as total_rows from information_schema.table_statistics \"\n",
        "                            f\"where database_name = '{database_name}' and table_name = '{table_name}'\")\n",
        "                result = cursor.fetchone()\n",
        "            # Check if the result is None\n",
        "            if result is None:\n",
        "                logging.warning(f'RowCounts missing for database:{database_name}, table: {table_name}')\n",
        "                return -1\n",
        "            else:\n",
        "                # Cache the row count and return it\n",
        "                table_row_count_cache[lookup_key] = result['total_rows']\n",
        "                logging.info(f\"fetched rowcount: {result['total_rows']}\")\n",
        "                return result['total_rows']\n",
        "    else:\n",
        "        logging.warning(f\"database field empty, so returning -1 for table:{table_name}\")\n",
        "        return -1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetch_rowcount(json_data):\n",
        "    \"\"\"\n",
        "    Extract table names and their corresponding rowcounts from a JSON string.\n",
        "\n",
        "    Args:\n",
        "    json_data (str): The JSON string containing table names and rowcounts.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing table names as keys and their rowcounts as values.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        data = json.loads(json_data)\n",
        "\n",
        "        # Define the JSONPath expression to extract rowcounts\n",
        "        expr = parse(\"$.*.rowcount\")\n",
        "\n",
        "        # Extract rowcounts for each table\n",
        "        matches = expr.find(data)\n",
        "\n",
        "        # Create a dictionary to store table names and rowcounts\n",
        "        table_rowcounts = {}\n",
        "        for match in matches:\n",
        "            # Extract the table name from the JSONPath match\n",
        "            table_name = match.full_path.left.fields[0]\n",
        "            rowcount = match.value\n",
        "            table_rowcounts[table_name] = rowcount\n",
        "\n",
        "        return table_rowcounts\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Raise an error if JSON parsing fails\n",
        "        logging.error(\"Invalid JSON data: \" + str(e))\n",
        "        raise ValueError(\"Invalid JSON data: \" + str(e))\n",
        "\n",
        "\n",
        "def extract_table_columns(comparison_expression, table_aliases):\n",
        "    \"\"\"\n",
        "    Extract left and right table columns from a comparison expression.\n",
        "\n",
        "    Args:\n",
        "    comparison_expression (str): The comparison expression.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the left and right table columns if they exist,\n",
        "           or None if only one side is present in the expression.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        match = re.match(r\"\\(`([^`]+)`\\.`([^`]+)` (?:=|>|<|>=|<=)+ `([^`]+)`\\.`([^`]+)`\\)\", comparison_expression)\n",
        "        if match:\n",
        "            left_tab = resolve_table_names((match.group(1) if match.group(1) else \"\"), table_aliases)\n",
        "            left_col = (match.group(2) if match.group(2) else \"\")\n",
        "            right_tab = resolve_table_names((match.group(3) if match.group(3) else \"\"), table_aliases)\n",
        "            right_col = (match.group(4) if match.group(2) else \"\")\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "        # exp like \"(`mq`.`keyword` > 0)\"\n",
        "        match = re.match(r\"\\(`([^`]+)`\\.`([^`]+)` (?:=|>|<|>=|<=)+ ([\\d.]+)\\)\", comparison_expression)\n",
        "        if match:\n",
        "            left_tab = resolve_table_names((match.group(1) if match.group(1) else \"\"), table_aliases)\n",
        "            left_col = (match.group(2) if match.group(2) else \"\")\n",
        "            right_tab = \"\"\n",
        "            right_col = \"\"\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "        # exp like \"(`mq`.`keyword` > '0')\"\n",
        "        match = re.match(r\"\\(`([^`]+)`\\.`([^`]+)` (?:=|>|<|>=|<=)+ ('.+')\\)\", comparison_expression)\n",
        "        if match:\n",
        "            left_tab = resolve_table_names((match.group(1) if match.group(1) else \"\"), table_aliases)\n",
        "            left_col = (match.group(2) if match.group(2) else \"\")\n",
        "            right_tab = \"\"\n",
        "            right_col = \"\"\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "        # exp like \"( 0 < `mq`.`keyword`)\"\n",
        "        match = re.match(r\"\\(([\\d.]+) (?:=|>|<|>=|<=)+ `([^`]+)`\\.`([^`]+)`\\)\", comparison_expression)\n",
        "        if match:\n",
        "            left_tab = \"\"\n",
        "            left_col = \"\"\n",
        "            right_tab = resolve_table_names((match.group(2) if match.group(2) else \"\"), table_aliases)\n",
        "            right_col = (match.group(3) if match.group(3) else \"\")\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "        # exp like \"(`mq`.`keyword` = NULL)\"\n",
        "        match = re.match(r\"\\(`([^`]+)`\\.`([^`]+)` (?:=|>|<|>=|<=)+ (.*?)\\)\", comparison_expression)\n",
        "        if match:\n",
        "            left_tab = resolve_table_names((match.group(1) if match.group(1) else \"\"), table_aliases)\n",
        "            left_col = (match.group(2) if match.group(2) else \"\")\n",
        "            right_tab = \"\"\n",
        "            right_col = \"\"\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "        # exp like \"(`mq`.`keyword` = 'NULL')\"\n",
        "        match = re.match(r\"\\(`([^`]+)`\\.`([^`]+)` (?:=|>|<|>=|<=)+ ('.*?')\\)\", comparison_expression)\n",
        "        if match:\n",
        "            left_tab = resolve_table_names((match.group(1) if match.group(1) else \"\"), table_aliases)\n",
        "            left_col = (match.group(2) if match.group(2) else \"\")\n",
        "            right_tab = \"\"\n",
        "            right_col = \"\"\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "\n",
        "        # exp like ( DATE_FORMAT(`mq`.`record_date`,'%Y') = `mt`.`year`)\n",
        "        match = re.match(r\"\\( ([A-Za-z_]+)\\(`([^`]+)`\\.`([^`]+)`\\,('.+')\\) (?:=|>|<|>=|<=)+ `([^`]+)`\\.`([^`]+)`\\)\",\n",
        "                         comparison_expression)\n",
        "        if match:\n",
        "            left_tab = resolve_table_names((match.group(2) if match.group(2) else \"\"), table_aliases)\n",
        "            left_col = (match.group(3) if match.group(3) else \"\")\n",
        "            right_tab = resolve_table_names((match.group(5) if match.group(5) else \"\"), table_aliases)\n",
        "            right_col = (match.group(6) if match.group(6) else \"\")\n",
        "            return left_tab, left_col, right_tab, right_col\n",
        "\n",
        "        return None, None, None, None\n",
        "    except Exception as ce:\n",
        "        logging.error(f\"Error extracting table columns from '{comparison_expression}': {ce}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def resolve_table_names(table_alias, table_alias_dict):\n",
        "    \"\"\"\n",
        "    Resolve the actual table name from a given table alias.\n",
        "\n",
        "    Args:\n",
        "    table_alias (str): The table alias to resolve.\n",
        "    table_alias_dict (dict): A dictionary mapping table aliases to actual table names.\n",
        "\n",
        "    Returns:\n",
        "    str: The resolved table name.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if table_alias in table_alias_dict:\n",
        "            return table_alias_dict[table_alias]\n",
        "        elif re.match(r'.*_[0-9]+$', table_alias):\n",
        "            return table_alias[:-2]\n",
        "        else:\n",
        "            return table_alias\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error resolving table name for alias '{table_alias}': {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def analyze_comparison(row):\n",
        "    \"\"\"\n",
        "    Analyze a comparison expression in a row and generate a suggestion based on table row counts.\n",
        "\n",
        "    Args:\n",
        "    row (dict): A dictionary representing a row of data with keys 'DATABASE_NAME', 'CMP_EXP', 'LEFT_TABLE',\n",
        "                'RIGHT_TABLE', and 'TBL_ROW_COUNTS'.\n",
        "\n",
        "    Returns:\n",
        "    dict: The input row dictionary updated with 'SUGGESTION', 'LEFT_TABLE_ROW_COUNT', and 'RIGHT_TABLE_ROW_COUNT'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        db_name = row['DATABASE_NAME']\n",
        "        cmp_element = row['CMP_EXP']\n",
        "        left_table = row['LEFT_TABLE']\n",
        "        right_table = row['RIGHT_TABLE']\n",
        "        #rowcount_dict = fetch_rowcount(row['TBL_ROW_COUNTS'])\n",
        "        rowcount_dict = parse_table_row_counts(row['PLAN_ID'],row['TBL_ROW_COUNTS'])\n",
        "\n",
        "        if rowcount_dict is None:\n",
        "            rowcount_dict = {}\n",
        "            if left_table:\n",
        "                rowcount_dict[f'{db_name}.{left_table}'] = fetch_table_row_counts(db_name, left_table)\n",
        "            if right_table:\n",
        "                rowcount_dict[f'{db_name}.{right_table}'] = fetch_table_row_counts(db_name, right_table)\n",
        "\n",
        "        suggestion = io.StringIO()\n",
        "\n",
        "        suggestion.write(f'For Expression: {cmp_element}. ')\n",
        "        left_row_count = None\n",
        "        right_row_count = None\n",
        "\n",
        "        if left_table is not None:\n",
        "            left_lookup_key = (left_table if db_name in left_table else db_name + '.' + left_table)\n",
        "            left_row_count = rowcount_dict.get(left_lookup_key, -1)\n",
        "\n",
        "        if right_table is not None:\n",
        "            right_lookup_key = (right_table if db_name in right_table else db_name + '.' + right_table)\n",
        "            right_row_count = rowcount_dict.get(right_lookup_key, -1)\n",
        "\n",
        "        if left_row_count is not None and right_row_count is not None:\n",
        "            if left_row_count < right_row_count:\n",
        "                suggestion.write(f\"{left_table} has fewer records, consider table size while optimizing.\")\n",
        "            elif left_row_count > right_row_count:\n",
        "                suggestion.write(f\"{right_table} has fewer records, consider table size while optimizing.\")\n",
        "            else:\n",
        "                suggestion.write(f\"The number of records is equal on both sides of the expression.\")\n",
        "        else:\n",
        "            suggestion.write(f\"Unable to determine row counts for comparison: {cmp_element}.\")\n",
        "\n",
        "        row['SUGGESTION'] = suggestion.getvalue()\n",
        "        row['LEFT_TABLE_ROW_COUNT'] = left_row_count\n",
        "        row['RIGHT_TABLE_ROW_COUNT'] = right_row_count\n",
        "        return row\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error analyzing comparison for row '{row}': {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def generate_datatype_mismatch_rep(conn, output_file, batch_size):\n",
        "    \"\"\"\n",
        "    Process database records fetched using a SQL query and generate a CSV report.\n",
        "\n",
        "    Args:\n",
        "    conn: The database connection object.\n",
        "    sql_query (str): The SQL query to execute.\n",
        "    HEADERS (list): A list of headers for the CSV report.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Starting database record processing...\")\n",
        "        lines = []\n",
        "        total_records_processed = 0\n",
        "        data_prep_start = time.perf_counter()\n",
        "        with conn.cursor() as cursor:\n",
        "                cursor.execute(MISMATCH_DATATYPE_ROWCOUNT_COMP_QUERY)\n",
        "                while True:\n",
        "                    database_records = cursor.fetchmany(batch_size)\n",
        "                    logging.info(f\"fetched {len(database_records)} rows\")\n",
        "                    if not database_records:\n",
        "                        break\n",
        "                    for row in database_records:\n",
        "                        try:\n",
        "                            parser = Parser(row['SQL_QUERY'])\n",
        "                            row['LEFT_TABLE'], row['LEFT_COLUMN'], row['RIGHT_TABLE'], row[\n",
        "                                'RIGHT_COLUMN'] = extract_table_columns(row['CMP_EXP'], parser.tables_aliases)\n",
        "                            row = analyze_comparison(row)\n",
        "                            lines.append(row)\n",
        "\n",
        "                            total_records_processed += 1\n",
        "                            if total_records_processed % 1000 == 0:\n",
        "                                logging.info(f\"Processed {total_records_processed} records...\")\n",
        "                        except Exception as ex:\n",
        "                            logging.debug(f\"while processing record: {row}\")\n",
        "                            logging.error(ex)\n",
        "\n",
        "        logging.debug(f\"total Processed {total_records_processed} records\")\n",
        "        logging.info(f'Data Preparation took {(time.perf_counter() - data_prep_start):.4f} seconds')\n",
        "        if total_records_processed > 0 :\n",
        "            report_write_start = time.perf_counter()\n",
        "            # with open(output_file, 'w') as csv_file:\n",
        "            #     writer = csv.DictWriter(csv_file, HEADERS, lineterminator='\\n', delimiter=',', extrasaction='ignore', dialect='excel')\n",
        "            #     writer.writeheader()\n",
        "            #     writer.writerows(lines)\n",
        "\n",
        "            wb = Workbook()\n",
        "            ws = wb.active\n",
        "            ws.append(list(HEADERS))\n",
        "\n",
        "            for line in lines:\n",
        "                #logging.debug(f\"{type(line)}\")\n",
        "                ws.append(list(line.values()))\n",
        "\n",
        "            wb.save(output_file)\n",
        "\n",
        "\n",
        "            logging.info(\"Writing to report completed\")\n",
        "            logging.info(f'Report Writing took {(time.perf_counter() - report_write_start):.4f} seconds')\n",
        "            return True\n",
        "        else:\n",
        "            logging.info('No records to write, skipping report')\n",
        "            return False\n",
        "\n",
        "        #logging.info(\"Database record processing completed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during database record processing: {e}\")\n",
        "        raise"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process dataframe and generate reports for each query in csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(query_csv_dataframe):\n",
        "    \"\"\"\n",
        "    Process a DataFrame containing query data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    query_csv_dataframe : pandas.DataFrame\n",
        "        The DataFrame containing query data.\n",
        "\n",
        "    \"\"\"\n",
        "    excel_report_file = f'{local_dir_path}/perf_troubleshoot_report.xlsx'\n",
        "    with pd.ExcelWriter(excel_report_file, engine=\"openpyxl\") as xlwriter:\n",
        "\n",
        "        for idx, row in query_csv_dataframe.astype(str).iterrows():\n",
        "            query_id = row['QueryID']\n",
        "            query_name = row['QueryName']\n",
        "            query = row['QueryTxt']\n",
        "\n",
        "            logging.debug(f'about to execute {query_name}')\n",
        "            xlwriter.book.create_sheet(query_name[:30])\n",
        "\n",
        "            try:\n",
        "                # Execute the query\n",
        "                result = execute_query(conn, query)\n",
        "\n",
        "                logging.info(f\"Fetched query ID: {query_id} NAME: {query_name}\")\n",
        "                # Fetch the template for the result page\n",
        "                template = fetch_url_content(template_url_base + 'Result-' + str(query_id) + '.template.html')\n",
        "                if not result:\n",
        "                    # If the result is empty, replace the template placeholder with an empty table\n",
        "                    final_content = template.replace('rstable', empty_result_table)\n",
        "                else:\n",
        "                    # If the result is not empty, format it as an HTML table and replace the template placeholder\n",
        "                    result_df = pd.DataFrame(result)\n",
        "                    result_df.columns = map(str.upper, result_df.columns)\n",
        "                    result_table_id = 'rstbl'\n",
        "                    result_table_content = result_df.to_html(table_id=result_table_id,\n",
        "                                                             index=False,\n",
        "                                                             classes=result_table_html_classes)\n",
        "\n",
        "                    final_content = template.replace('rstable', result_table_content)\n",
        "                    result_df.to_excel(xlwriter, sheet_name=query_name, index=False)\n",
        "\n",
        "                # Write the final content to an HTML file\n",
        "                report_file = f'{local_dir_path}/{query_id}.html'\n",
        "                with open(report_file, 'w') as writer:\n",
        "                    writer.write(final_content)\n",
        "\n",
        "\n",
        "            except Exception as curr_iter_err:\n",
        "                # If an exception occurs during query execution, log the error and show a warning message\n",
        "                logging.error(f\"Error executing query ID: {query_id}, NAME: {query_name}: {curr_iter_err}\")\n",
        "                logging.exception(\"Exception details\")\n",
        "                show_warn(f\"Error executing query ID: {query_id}, NAME: {query_name}\")\n",
        "\n",
        "            logging.info(f'process completed for ID:{query_id} Name:{query_name}')\n",
        "\n",
        "\n",
        "    logging.info('Result Pages are generated')\n",
        "    logging.info(f'Excel Report perf_troubleshoot_report.xlsx is generated')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to clean up generated directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_dir(p_dir_path, p_archive_file_path):\n",
        "    \"\"\"\n",
        "    Clean the local directory by removing all HTML files and the archive file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p_dir_path : str\n",
        "        The path to the local directory.\n",
        "    p_archive_file_path : str\n",
        "        The path to the archive file.\n",
        "\n",
        "    \"\"\"\n",
        "    # Remove the archive file\n",
        "    try:\n",
        "        os.remove(p_archive_file_path)\n",
        "        logging.info('Local archive file removed')\n",
        "        logging.info('about to clean previous generated files in local dir')\n",
        "        shutil.rmtree(p_dir_path)\n",
        "    except OSError as e:\n",
        "        logging.error('Clean up failed')\n",
        "        execution_success = False\n",
        "        error_msg = 'clean up failed'\n",
        "        print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
        "        raise Exception(f'Failed to clean up {str(e)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "if connection_url.endswith('/'):\n",
        "    show_warn('Database not selected. Please select from dropdown in top of web page')\n",
        "else:\n",
        "    execution_success = True\n",
        "    final_file_path = None\n",
        "    error_msg = None\n",
        "    try:\n",
        "        if verify_stage_area():\n",
        "            # Establish a database connection, use dict as results_type\n",
        "            conn = s2.connect(results_type='dict')\n",
        "            logging.info('Database Connection establised')\n",
        "            # Load query data from a csv file into a pandas DataFrame.\n",
        "            queries_df = load_query_data(url=query_data_url)\n",
        "            logging.info('Query Data loaded')\n",
        "             # Create a local directory for storing result files\n",
        "            local_dir_path = (s2_workspace_name + local_output_dir_suffix)\n",
        "            path = Path(local_dir_path)\n",
        "            path.mkdir(exist_ok=True)\n",
        "\n",
        "            process_dataframe(queries_df)\n",
        "\n",
        "\n",
        "            mismatch_report_file_path = f\"{local_dir_path}/datatype-mismatch-comparision-report.xlsx\"\n",
        "            report_ready = generate_datatype_mismatch_rep(conn, output_file=mismatch_report_file_path, batch_size=1000)\n",
        "\n",
        "            copy_index_file(local_dir_path, template_url_base)\n",
        "\n",
        "            # Create a zip archive of the result files\n",
        "            final_file_path = s2_workspace_name + '_PERF_REPORT_' + my_timestamp + '.tar.gz'\n",
        "\n",
        "            zip_success = make_tarfile(final_file_path, local_dir_path)\n",
        "\n",
        "            logging.info('archive created')\n",
        "            # Upload the zip archive to the stage area\n",
        "            if zip_success:\n",
        "                try:\n",
        "                    uploaded_obj = s2_workgroup_stage.upload_file(local_path=final_file_path,\n",
        "                                                                  stage_path=f'{stage_folder_path}/{final_file_path}')\n",
        "                    logging.info(f'Upload success. Path: {uploaded_obj.abspath()} ')\n",
        "                    print(f'File uploaded to STAGE AREA: {uploaded_obj.abspath()}')\n",
        "                    logging.info('Upload success')\n",
        "                except Exception as e:\n",
        "                    # If an exception occurs during the upload process, log the error\n",
        "                    execution_success = False\n",
        "                    logging.error(f'Failed during upload process{e}')\n",
        "                    error_msg = 'File Upload failed'\n",
        "\n",
        "                clean_dir(local_dir_path, final_file_path)\n",
        "                logging.info('Local files cleaned')\n",
        "\n",
        "            else:\n",
        "                # If creating the zip archive fails, set execution_success to False and log the error\n",
        "                logging.error('Failed to create archive')\n",
        "                execution_success = False\n",
        "                error_msg = 'Failed to create archive'\n",
        "\n",
        "        else:\n",
        "            # If verifying the stage area fails, set execution_success to False and log the error\n",
        "            logging.info(\"Stage Area Verification Failed. Exiting.\")\n",
        "            print('Script execution Failed')\n",
        "            execution_success = False\n",
        "            error_msg = 'Failed to create missing stage area path or it is not writeable'\n",
        "    except Exception as e:\n",
        "        execution_success = False\n",
        "        logging.error(f\"An error occurred: {e}\")\n",
        "        logging.exception(\"Exception details\")\n",
        "        error_msg = f'Exception occured. {str(e)}'\n",
        "\n",
        "    # Display a success or error message based on the execution success\n",
        "    if execution_success:\n",
        "        #show_success(\"Files are uploaded to Stage\")\n",
        "        show_success(generate_stage_link('Upload to stage success, File: ', stage_folder_path, final_file_path))\n",
        "\n",
        "    else:\n",
        "        show_error(error_msg)\n",
        "\n",
        "    logging.info(f'Script execution completed sucessfully: {execution_success}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important NOTE**\n",
        "\n",
        " - Actions suggested suit most of performance improvement scenarios, Still we would encourage to test and verify before applying on prod environemnts\n",
        " - To use notebook as scheduled one, we have to modify python code to refer configuration from table instead of user input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a3a54b-ea69-4827-bc8b-81cb767e6a84",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
